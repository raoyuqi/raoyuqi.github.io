{"meta":{"title":"奇遇的博客","subtitle":"","description":"","author":"John Doe","url":"https://raoyuqi.github.io","root":"/"},"pages":[{"title":"文章分类","date":"2023-07-19T12:40:46.000Z","updated":"2023-07-19T13:37:47.381Z","comments":true,"path":"categories/index.html","permalink":"https://raoyuqi.github.io/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"纹理压缩原理","slug":"unity/性能优化/纹理压缩原理","date":"2023-09-26T06:43:57.000Z","updated":"2023-09-27T06:13:15.180Z","comments":false,"path":"2023/09/26/unity/性能优化/纹理压缩原理/","link":"","permalink":"https://raoyuqi.github.io/2023/09/26/unity/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/%E7%BA%B9%E7%90%86%E5%8E%8B%E7%BC%A9%E5%8E%9F%E7%90%86/","excerpt":"","text":"1. 前言纹理是2D图像，可以在不增加几何复杂度的情况下通过映射在3D表面来提升表现质量。纹理中的每一个像素又称为 纹素，这是纹理的基本单位。在游戏项目中，绝大部分内存都是被纹理所占用，因此对内存大小和带宽有很高的需求。 2. 为什么要压缩纹理 随机访问：由于纹理访问的模式高度随机（只有在渲染时被用到的部分才需要访问到，且无法提前预知其顺序），且场景中相邻的像素在纹理中不一定是相邻的，如下图，而标准的压缩算法 ( RLE, LZW, Deflate) 以及常用的图片压缩格式 ( JPEG, PNG TIFF) 都是对图片 整体压缩，因此无法在不解压整张图的情况下直接访问某个纹素，无法发挥GPU的并发优势。 带宽：带宽是发热的元凶，材质通常都是以压缩状态存储在内存中并传输至 GPU。解压过程只发生在 GPU 上，在渲染3D场景时，会有大量的贴图被传输到GPU，合理的压缩纹理可以降低带宽。 3. 压缩特性 随机访问：支持GPU随机访问任意元素，因此几乎所有的压缩器都拥有固定的压缩比，这也就意味着有损压缩。 高解压速度：通常编码速度慢没关系，因为通常纹理压缩只需要在游戏打包时进行一次，不影响游戏运行时的体验。但解码速度必须足够快，这样才能最大程度降低对渲染性能的影响。 高压缩比：决定带宽占用的主要因素，压缩比通常用比特率或每像素平均比特数 (bpp - byte per pixel) 来表示，常见的为2~8bpp。一般RGB原生纹理的像素指24位，4bpp表示每像素占4位，所以也可以认为4bpp表示压缩比为6:1。 block：图片会被分割成小块，通常为 4x4 纹素，太大对 cache 命中不友好。为了减少延迟，压缩块的大小最好小于内存总线的宽度（现代图形处理器的内存总线通常从 64 位到 512 位）。 质量：解压后的纹理拥有可以接受的视觉效果。 4. 纹理格式4.1 无压缩格式不压缩的纹理格式，可以被GPU直接采样，缺点就是内存占太多，占用带宽： 4.2 常用纹理压缩格式4.2.1 DXT所有 S3TC 系列格式都使用 4x4 的块。 原名S3TC (S3 Texture Compression) 最早由S3公司研发并取得专利，由于版权专利一般用于Window平台。微软将 S3TC 引入其 3D 图形API DirectX 6.0，名称为 DXT1，不支持透明通道。带有 Alpha 通道的纹理的修改即为 DXT2-DXT5。 从 DirectX 10 开始，这些格式又被称为 BC1-BC3 (块压缩) 。同时添加了两种新的格式： BC4 和 BC5，随着 DirectX 11 的发布 ，又引入了两种新的格式：BC6H， 第一种是针对高动态范围 (HDR - high dynamic range) 纹理的标准格式；而 BC7，则用于处理非常高质量的压缩。 1. BC1块(S3TC/DXT1) 将纹理划分为 4x4 的block，存储 64 位数据 在16个纹素中，选取两个作为基色，表示为 $c0, c1$，以 RGB565 格式存储，即红色占5bit， 绿色占6bit，蓝色占5bit 16个2位选择器值，一个像素最终的颜色值由 $c0, c1$ 和对应的2位选择器值决定 能够达到 4bpp 的压缩比：两个基色 (162bit=32bit)，16个选择器 (162bit=32bit)，将原始数据 (4x4x32=512bit)压缩为 64bit，压缩比4bpp。 块示例： 最终的颜色值混合公式（选择器的值不同，混合结果也不同），每个像素可以根据选择器值合并color0和color1值来解码： 2. BC2块(DXT2/DXT3) BC1格式可以处理24位的RGB纹理，但不适用32位的RGBA888，Alpha 通道可以用于存储透明度、高光或其他材质属性。因此BC2在BC1的基础上支持了透明度，块数据： 将4x4共16个像素存储为128位数据 其中64位alpha通道，每纹素4位 另外64位和BC1一致 实际上，BC2 相当于压缩了RGB通道的格式为RGBA8884的纹理。它的透明度只有4位共16种数值，相比DXT5有些辣鸡基本没人用。DXT2和DXT3的差别在于是否是预乘过颜色。 3. BC3块(DXT4/DXT5) BC3 的块 (与 BC2 类似) 由两段64位数据组成：一段是 Alpha 数据，另一段是颜色数据。颜色部分同样使用了 BC1 的格式，但 Alpha 部分则以压缩形式存储，见下图。所以数值范围更广，是比较常见的格式。Alpha 通道的压缩与 DXT1 类似：有两个 8 位精度的端点，3 位的索引表，因此能在局部调色板上 8 选 1。 4. BC4块(ATI1/3Dc+)/BC5块(ATI2/3Dc) BC4和BC5在D3D10中可用，只能存储一个/两个颜色通道。 5. BC6H块和BC7块 BC6H和BC7在D3D11中可用，BC6H 格式旨在压缩高动态范围的纹理，仅支持不带alpha的GRB图像。它将 4x4 共16个纹素存储为128位数据，其中包括两个48位的RGB值（16:16:16），每个颜色分量都是带符号浮点值（1 个符号位 + 5 个指数位 + 10 个尾数位），以及16个2位索引表。 BC7相比其它有点特殊，虽然它也是将 4x4 共16个纹素存储为128位数据，但它的最低有效位为mode位，（最低有效位即最低的非0位）根据不同模式，颜色值的存储格式不同，是否有a值或a值的存储格式也不尽相同，是一种比较灵活的存储格式，但这也意味着解码所带来更多的消耗。 4.2.2 ETC系列最初为移动平台设计，现在是Android平台的标准压缩方案。核心思想：基于人眼对于亮度的敏感要高于颜色的原理，编码时将色度和亮度分开存储，解码时将亮度偏移叠加到基颜色上还原，如下图： 1. ETC1 将纹理拆成 4x4 的block 再分为横向或纵向的 2x4 或 4x2 的两个子块，用一个 “flip” 位指定子块的垂直或水平排列 为两个子块指定一个基色，分两种模式，差分模式(Differential)和独立模式(Individual)，用1bit标识模式： Individual：diff=0，两块颜色差异很大时用。 用RGB444存储2个子块的基色 Differential：diff=1，两块颜色接近时时用，sub-block1 的基色为 $c1$，与sub-block2的基色差值为 $c2$，所以sub-block2的 $base color=c1+c2$，$c1$ 用RGB555表示，$c2$ 用RGB333表示，共占用24bits，精度比individual模式更高。 每个子块包含一个3位的修饰表索引，对应于8种修饰值，用来亮度偏移 ETC1的块布局： 3bit 的修饰表索引对应于8种修饰值： 一个子块由1个基本颜色值和4个修饰值可以确定出4种新的颜色值： base_color + RGB(modifier0, modifier0, modifier0) base_color + RGB(modifier1, modifier1, modifier1) base_color + RGB(modifier2, modifier2, modifier2) base_color + RGB(modifier3, modifier3, modifier3) 最终颜色根据另外32位数据中包含的 16个2bit 选择器数据从4个颜色数据中选出一个。 块解码示例： 缺点： ETC1不支持透明通道，透明通道需要单独存储；子块只有1个Base color，如果颜色分布不均匀，色度范围广，会丢失很多颜色，所以对有渐变的纹理支持很差。 2. ETC2 兼容ETC1，且改进了ETC1算法的缺点，并支持透明通道。ETC2的改变主要是针对 Differential 模式，当基色和偏移的总和溢出了 5bit 的有效范围 [0, 31] 时，是无意义。 超过范围时，使用其它编码模式，以提高精度。 ETC2常用压缩方式有三种： 将16 x RGB24bits的block，压缩为64bit 将16 x (RGBA24bit +1bisAlpha) block，压缩为64bit 将16 x RGBA32bit block，压缩为128bit 模式： Individual模式： D=0，算法与ETC1的Individual模式一样 Differential模式： D=1，算法与ETC1的Differential模式一样 T模式： 2个块颜色相近，D=1，R通道溢出，同时忽略G，B通道是否会溢出，此时使用T模式压缩纹理。T模式的颜色分布如下图所示，大部分颜色沿着直线分布，但部分像素颜色差别较大，总体呈T形分布。首先，计算得到2个Base Color A和B（用RGB444表示），然后沿着大部分颜色的分布直线，通过加减距离d，来修改A的亮度，得到颜色 C0=A-(d, d, d) 和 C1=A+(d ,d ,d)。其中R通道并未连续分布，用的是原来R5和R3中的低两位。得到4个Base Color A，B，C0，C1，而sub-block中的纹素就是着4种颜色中的一种，所以需要2位编码。 H模式： R通道没有溢出，且G通道溢出，不论B通道是否溢出，此时选择H模式。H模式也有4个Base Color，颜色分布如下图，这些颜色在 RGB 空间中形成 H 形，适用于那些，颜色位于两条线上的块。同样先计算2个Base Color A和B，然后通过加减d，偏移亮度，得到4个新的Base Color C0，C1,C2,C3, 通过如同T模式一样，用0-31位构建16个像素得2bits索引，选择4个基础颜色之一。 Planar模式： 当R和G通道均没有溢出，且B通道溢出，选择Planar模式。该模式使用3个基色 C0、Cv、Ch如下图，且三种颜色都以RGB676格式存储，所有纹素的颜色使用线性滤波方程 $C(x, y)=x(C_H – C_0)/4 + y(C_v-V_0)/4 + C_0$ 便可以得到。 ETC2的RGBA8881 丢弃独立模式，因此仅仅通过是否溢出来区分 T、H 和 Planar 模式。Diff-bit 用于指定差分子模式。D=1时是差分模式，像之前一样解压，D=0时，亮度偏移表索引 10 是透明度，如下图。将RGBA8881，同样压缩为4bit/像素，T、H 和Planar 块的解码过程保持不变。 4.2.3 EAC核心原理与ETC相同，但它只用于单通道或双通道数据，OpenGL ES 3.0和OpenGL 4.3后的设备大部分支持，但由于安卓平台的兼容性，一般不建议用单双通道贴图。 4.2.4 PVRTC系列是由 Imagination Technologies 持有专利，专为 PowerVR 图形核心系列设计的。 用于 Apple 移动设备，例如 iPhone、iPad和部分PowerVR的安卓机支持。可能是这几种压缩格式中最封闭的技术。 与 DXT 和 ETC 不同，PVRTC 并非基于块的编解码器。核心思想与小波压缩有一些共通之处，即整张图像被分成低频和高频信号。低频信号由两张低分辨率图像 A 和 B 呈现，在两个维度上都按比例缩小了 4 倍，高频信号是完整分辨率低精度的调制信号 M。要解码整张图像，首先应放大图像 A 和 B，然后使与调制信号 M 混合，其中调制信号 M 指定了每个纹素的混合权重，如下图： PVRTC算法平滑度更好，没有块伪影，但是可能会丢失那些高频细节。PVRTC 和 PVRTC2 两个版本都有 4bpp 和 2bpp 模式。为了不混淆，可用别名区分：PVRTC 4bpp、PVRTC 2bpp、PVRTC2 4bpp、PVRTC2 2bpp。 1. PVRTC 4bpp 将纹理分成4x4的block，每个块由图像 A 的一个像素、图像 B 的一个像素和相应的 4x4 调制系数区域组成，如下图： A 和 B 颜色都能以 RGB 或 RGBA 格式存储，两个颜色字段中的最高位决定使用哪种格式。其中 A 颜色只有 15bit，因此 A 颜色用 RGB554 或 ARGB3443 编码，B 颜色可用 RGB555 或 ARGB3444 编码。调制区块对应每个纹素对应的调制权重，每个权重2bits。Mode位用于设置Alpha通道,以及2bpp设置。 解码示例图： 解码的时候通过使用使用双线性滤波方法图像 A 和 B，然后通过 4 个相邻 PVRTC 块解码 5x5 区域颜色。AB 纹理的混合权重就是 M 纹理，每个纹素对应一个 2bit 权重，如下图。在使用 1bit alpha 的情况下 “10” 留给透明度作权重。 2. PVRTC 2bpp 与 4bpp 算法相似，使用相同的块布局，如下图。不同之处在于，图像 A 和 B 在水平维度上又缩小了一半，且 32 位调制字段必须保存 8x4 纹素的调制信息。Mode位指定了调制信息的模式，M=0，每纹素对应1bit调制权重，M=1，每2个纹素对应2bits调制信息，并且像国际象棋格子一样错分配。 PVRTC 2bpp 解码的方式和 4bpp 一致： 3. PVRTC2 4bpp PVRTC2 增强了压缩质量，并消除了 PVRTC 的一些不足之处。例如，PVRTC2 支持 NPOT 纹理 (Non Power Of Two) ，两个维度的分辨率都可以不为 2 的幂。在 PVRTC 中，基色 A 和 B 各自可以独立选择使用 RGB 或 ARGB 存储格式，因此，在 PVRTC2 中仅使用一个位 (Opacity) 来指定两种颜色的格式，第二个位 (Hard) 用于编码新模式然而通常是两种颜色具有相同格式。PVRTC2 块的布局如下： “Hard”位和“Mode”位一起能够表示四种解码模式： 解码方式： 标准双线性与PVRTC相同，punch-alpha略有不同 新的“Non-interpolated”(无插值)模式，将 “Hard “位设置为 “1”，简化了纹理图集的创建。在这个模式下 A，B 纹理被放大，但不插值，块内像素具有相同颜色，当处理颜色不连续纹理时效果较好。随后的解码过程与PVRTC中的解码过程相同。 “Local-palette”（局部调色板）模式下，A，B不混合，局部调色板由来自四个相邻的 PVRTC2 块的 A 、B 对填充，共8种颜色，但一个索引的大小只有2 比特，这意味着这 8 种颜色中只有 4 种可用于每个特定的纹素。 选择4种颜色混合，作为某纹素的最终颜色，假如4个相邻的block(P，Q，R，S)，对应的A，B纹理颜色分别为：Pa、Qa、Ra、Sa和Pb、Qb、Rb、Sb ，那么混合方式如下图，可以看出混合不仅取决于权重，还取决于纹素在block中的位置。 4.2.5 ASTC所有ARM图形处理器都有ASTC硬件支持，由 ARM 和 AMD 联合开发，2012年发布，专利归ARM所有，是较新的一种压缩格式，唯一一个不受专利权影响的压缩格式，完全开放免费。 优势： 较高的灵活性：支持1-4分量的贴图，虽然单通道纹理也可以使用BC7、PVRTC2或ETC2来存储，但空通道上大量bit被浪费掉了 支持 LDR 和 HDR，BC6H可用于HDR纹理压缩，但它不支持alpha通道 在通道之间数据无相关性的情况下，拥有可以接受的质量。这对于法线图和 RGBA 图像来说非常重要 跨平台兼容，IOS、安卓都支持。PVRTC 只支持 iOS 设备，BC6H/BC7 不支持移动设备，ETC 不支持桌面级 GPU 比特率/质量比的灵活性，根据纹理类型（因为不同图像的可压缩性是不同的），和可接受的压缩伪影，选择不同的压缩比例。其他压缩格式没有这么灵活的选择，且同一压缩比例时，ASTC的纹理质量几乎都优于其他纹理压缩格式 支持 2D 和 3D 纹理 1. 存储字母 尽管每个像素的颜色和权重，在概念上是浮点数，但一般都不会以浮点数存储，否则将占用太多空间。为了减小存储大小，这些值必须在压缩过程中量化。例如，为每个像素存储在 [0.0, 1.0] 区间的5个权重：0.0，0.25，0.5，0.75，1.0，可以使用整数值0-4表示存储中的这五个量化值。 在一般情况下，如果选择量化N个级别，需要能够有效地存储包含N个符号的字母表中的字符。每个 N 进制的数，可以用数量为 log2(N) 的二进制bit存储，如果要存5进制数值，那么需要 $log^{2⁡5} \\approx 2.32bits$，但是二进制存储需要四舍五入到3位。这浪费了22.3%的存储容量（3 - 2.32）/ 3）。 下图展示使用简单二进制编码存储任意进制数值造成的浪费，可以看出等于2的整数次幂时没有空间浪费，离2的整数次幂越远，浪费越多： 2. Quints and trits quint指5进制的数值，trit指3进制的数值，相比用3位2进制bits表示一个5进制数值，更有效的解决方案是将三个quint字符打包在一起。3位5进制数具有 $5^3(125)$ 种组合，包含 $log_2^{125} \\approx 6.97bits$ 信息。可以将这三个整数字符存储在7位中，并且存储浪费仅为0.5%。 类似的，可以将5位trit，$3^5 = 243 $，包含7.92bits的二进制信息，可以将这5个trit字符存储在8位中，并且只有1%的存储浪费。 3. 有界整数序列编码(Bounded Integer Sequence Encoding (BISE)) 可以实现任意进制的数值序列存储，允许使用最多256个符号的任意字母存储字符序列。每个字母大小都以最节省空间的位、三位数和五位数进行编码，极大提高空间利用率。 包含最多 $(2^N - 1)$ 个符号的字母可以使用每个字符N位进行编码 包含最多 $3 \\times (2^N - 1)$ 个符号的字母可以使用n位(m)和每个字符的三位数(t)进行编码，并使用公式 $((t \\times 2^N) + m)$，(m是个二进制，t是一个trit) 包含多达 $5 \\times (2^N - 1)$ 个符号的字母可以使用n位(m)和每个字符一个整数(q)进行编码，并使用公式 $((q \\times 2^N) + m)$，(m是二进制，q是一个quint) 压缩器选择为正在存储的字母大小产生最小存储的选项。有些使用二进制，有些使用位和三位数，有些使用位和五位数。下图显示了BISE存储相对于二进制存储的效率增益，会发现BISE的效率要高得多： ASTC 压缩方案使用 BISE 对颜色端点和插值权重进行编码，如下图： 4. Block sizes ASTC格式的块为固定大小的128位，但是，ASTC允许开发人员从一系列块大小中进行选择，以便在图像质量和大小之间进行细粒度权衡： 5. 颜色 block的颜色数据被编码为两个颜色端点之间的梯度。每个texel沿着这个梯度选择一个位置，然后在解压过程中进行插值，得到每个像素的颜色。ASTC支持16种颜色端点编码方案，称为端点模式。可以以block位单位设置端点颜色：如通道数量（luminance, luminance+alpha, RGB, or RGBA）、编码方式（direct, base+offset, base+scale, or quantization level）、数据范围（low dynamic range or high dynamic range）。下图显示了不同texel权重的插值： 6. 颜色分区 单一颜色梯度不能准确地表示所有不同的纹理颜色值。ASTC允许一个块定义多达四种不同的颜色梯度，称为分区，并可以将每个文本分配到单个分区。下图显示了分区索引如何指定每个texel使用的颜色梯度： ASTC 的分区模式是用一个特殊的哈希函数生成的，它为每个纹素分配一个分区索引。这个函数将纹素在瓦片中的位置、分区ID、瓦片大小和分区数量作为输入，并输出一个分区索引。该函数可以用硬件实现。 7. 权重 每个block可以设置像素权重和权重的BISE量化等级，对于压缩像素少的block, 可以每像素都存储权重，用2bit，3bit甚至4bit。但当每块压缩的像素增多时，比如每个block压缩12x12像素，即使每个像素1bit权重，也不够用，就会存储一个低分辨率的权重网格，比如为12x12的块，存4x6的权重网格，然后解压缩时通过双线性插值得到每像素的权重。 有些情况下，所有颜色通道用单一权重就可以，对于特殊的纹理，单一通道就会有问题。如，RGBA纹理，透明通道的变化和RGB并不相关；法线纹理，x,y向量值也是各自独立变化的。ASTC提供了双权重模式，每个block可以单独设置, 但是因为存储空间原因，选择双权重，就不能选择4个颜色分区。因为没有足够的位来同时存储一个额外的权重平面和一组额外的颜色端点。 8. 最优选择 ASTC在压缩时，会将所有配置数据 (网格大小、分区数量、端点格式) 存储在一个压缩块中，会牺牲一些颜色数据位，导致可能降低图像质量，但这种方法带来了很好的灵活性，并且大大增强了压缩质量。ASTC 允许在每个块中进行不同的比特权衡，任意block都可以在分区、端点和权重之间的分布选用最合适的比特分布进行编码。事实上，即使在较低的比特率下，ASTC也能够提供比 PVRTC、BC1-BC5 和 ETC 更好的质量。 9. 块结构 全局解码参数，这些参数对任何特定的纹理都是一样的。因此，没有必要在压缩块中存储这些。例如，ETC的亮度修正表, ASTC相关的动态范围（LDR/HDR），纹理尺寸，瓦片尺寸，颜色空间。 Block 包含的内容有： 权重网格大小 权重范围，用于BISE解码 权重值 分区的数量 分区模式 ID 颜色端点模式 颜色端点数据 平面的数量 (1或2) 平面到通道的分配 纹理可以被编码为单通道、双通道、三通道或四通道图像。但解码后的值总是以 RGBA 格式输出。在LDR sRGB 模式下，颜色值以 8 位整数返回，否则以 16 位浮点数返回。下图展示了 ASTC 块的布局: 除了 “BlockMode “和 “Part “字段，所有字段的长度都是可变的。 Part 字段：指定分区的数量，在双平面模式下，分区的数量必须是1、2或3。 BlockMode 字段：指定了平面数、权重范围和权重网格的大小。 ConfigData, MoreConfigData： 指定每个端点对的端点模式，和端点颜色。共有16种编码模式：10 种 LDR 格式和 6 种 HDR 格式。HDR 纹理可以使用其中任意一种。 10. 小结 ASTC有多种灵活的压缩比，如果增加alpha通道，同等压缩比，质量会下降。常用4x4、5x5、6x6的格式，8x8的质量太低了。 5. 总结 Android：ETC2 完全取代了 PACKMAN 和 ETC 格式，ASTC在Android 5.0/OpenGL ES 3.1后支持 IOS：PVRTC 被 PVRTC2 所取代，iPhone6以上（包含）都支持ASTC，6以下可以选择PVRTC2 PC： BC7能够为RGB和RGBA纹理提供最好的质量，可以完全取代BC2和BC3 BC1 可用于较低质量的RGB压缩 BC4 和 BC5 用于单通道和双通道纹理 BC6H 用于 HDR 纹理 所有压缩格式的简要描述如下，灰色框表示已经过时了或基本不用了： 6. 参考 TEXTURE COMPRESSION TECHNIQUES The-ASTC-format Unity Blog TextureImporterOverride 7. 扩展 compressed-gpu-texture-formats-part-1 compressed-gpu-texture-formats-part-2 compressed-gpu-texture-formats-part-3 ASTC纹理压缩格式详解","categories":[{"name":"Uniy","slug":"Uniy","permalink":"https://raoyuqi.github.io/categories/Uniy/"},{"name":"性能优化","slug":"Uniy/性能优化","permalink":"https://raoyuqi.github.io/categories/Uniy/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"}],"tags":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"Uniy","slug":"Uniy","permalink":"https://raoyuqi.github.io/tags/Uniy/"},{"name":"性能优化","slug":"性能优化","permalink":"https://raoyuqi.github.io/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"合并Mesh","slug":"合并Mesh","permalink":"https://raoyuqi.github.io/tags/%E5%90%88%E5%B9%B6Mesh/"}]},{"title":"Pixels Per Unit","slug":"unity/mix/Pixels Per Unit","date":"2023-09-26T03:27:33.000Z","updated":"2023-09-26T04:01:46.120Z","comments":false,"path":"2023/09/26/unity/mix/Pixels Per Unit/","link":"","permalink":"https://raoyuqi.github.io/2023/09/26/unity/mix/Pixels%20Per%20Unit/","excerpt":"","text":"1. Pixels Per UnitPixels Per Unit可以在在资源导入设置： 这个值默认是100，可以自由修改，由于原图只有 16 x 16，此处将默认 100 修改为 16。作用是设置Unity中一个单位显示多少个像素，这里也就是显示16个像素。 创建一个正交相机，size设为 0.5，效果如下： 可以看到正好图片高度正好铺满 1 个单位，验证了Pixels Per Unit的作用，即设置一个单位显示多少个像素。 2. Reference Pixels Per Unit这是Canvas中的设置，用来控制UI的大小，UGUI源码对Image的计算方式为： 1234567891011121314151617181920212223242526272829// 核心代码private float m_CachedReferencePixelsPerUnit = 100;public float pixelsPerUnit&#123; get &#123; float spritePixelsPerUnit = 100; if (activeSprite) spritePixelsPerUnit = activeSprite.pixelsPerUnit; if (canvas) m_CachedReferencePixelsPerUnit = canvas.referencePixelsPerUnit; return spritePixelsPerUnit / m_CachedReferencePixelsPerUnit; &#125;&#125;public override void SetNativeSize()&#123; if (activeSprite != null) &#123; float w = activeSprite.rect.width / pixelsPerUnit; float h = activeSprite.rect.height / pixelsPerUnit; rectTransform.anchorMax = rectTransform.anchorMin; rectTransform.sizeDelta = new Vector2(w, h); SetAllDirty(); &#125;&#125; 通过代码可以推出公式：UI大小 = 原图大小(Pixels) / (Pixels Per Unit / Reference Pixels Per Unit)，例：目标分辨率为 2000 x 1125，Pixels Per Unit为16，Reference Pixels Per Unit为100，通过公式可得出图分辨率为 320 x 180。 总结： Pixels Per Unit设置每单位填充像素 Reference Pixels Per Unit设置参考像素，每单位填充像素小于参考像素，会将UI方大至参考像素大小","categories":[{"name":"Uniy","slug":"Uniy","permalink":"https://raoyuqi.github.io/categories/Uniy/"}],"tags":[{"name":"Uniy","slug":"Uniy","permalink":"https://raoyuqi.github.io/tags/Uniy/"},{"name":"Mix","slug":"Mix","permalink":"https://raoyuqi.github.io/tags/Mix/"}]},{"title":"Mesh合并","slug":"unity/性能优化/Mesh合并","date":"2023-09-23T03:40:42.000Z","updated":"2023-09-24T07:05:33.847Z","comments":false,"path":"2023/09/23/unity/性能优化/Mesh合并/","link":"","permalink":"https://raoyuqi.github.io/2023/09/23/unity/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/Mesh%E5%90%88%E5%B9%B6/","excerpt":"","text":"1. 前言要在屏幕上绘制游戏对象，引擎必须向图形 API（例如 OpenGL 或 Direct3D）发出绘制调用。绘制调用通常为资源密集型操作，图形 API 为每次绘制调用执行大量工作，从而导致 CPU 端的性能开销。此开销的主要原因是绘制调用之间的状态变化（例如切换到不同材质），而这种情况会导致图形驱动程序中执行资源密集型验证和转换步骤。 Unity可以通过以下方法进行优化： 动态批处理：对于足够小的网格，此方法会在 CPU 上转换网格的顶点，将许多相似顶点组合在一起，并一次性绘制它们。 静态批处理：将静态（不移动）游戏对象组合成大网格，并以较快的速度渲染它们。 但是，也有一些缺点；静态批处理会导致内存和存储开销，动态批处理会产生一些 CPU 开销。 除此之外还可以利用Unity提供的 Mesh.CombineMeshes API在运行是合并网格，最近在优化具有换装需求的项目时正好使用到。 2. 实现2.1 流程前提：使用相同材质 收集需要合并的Texture 收集需要合并的UV信息 合并根节点下的Mesh、Texture、UV 2.2 收集需要合并Texture每个部位用到的Texture不同，因此先合并贴图： 12345678910111213141516171819202122232425/// &lt;summary&gt;/// 收集组件的贴图/// &lt;/summary&gt;/// &lt;param name=&quot;model&quot;&gt;模型根节点&lt;/param&gt; public List&lt;Texture2D&gt; CollectCombineTextures(GameObject model)&#123; // 模型中所有的SkinnedMeshRenderer var skinnedMeshRenderers = model.GetComponentsInChildren&lt;SkinnedMeshRenderer&gt;(); // 需要合并的贴图 var textureList = new List&lt;Texture2D&gt;(); // 遍历SkinnedMeshRenderer for (int i = 0; i &lt; skinnedMeshRenderers.Length; i++) &#123; var skinMeshRenderer = skinnedMeshRenderers[i]; var mat = skinMeshRenderer.sharedMaterial; if (mat == null) continue; // var texture2d = mat.GetTexture(&quot;_BaseMap&quot;) as Texture2D; var texture2d = mat.mainTexture as Texture2D; textureList.Add(texture2d); return textureList; &#125;&#125; 2.3 收集UV数据在本次实践中，shader用到了uv2，因此需要一起合并，类似可以拓展到uv3，uv4… 123456789101112131415161718public List&lt;Vector2[]&gt; CollectCombineUVList(GameObject model, List&lt;Vector2[]&gt; uv1List, ref int uv1Count, List&lt;Vector2[]&gt; uv2List, ref int uv2Count)&#123; // 模型中所有的SkinnedMeshRenderer var skinnedMeshRenderers = model.GetComponentsInChildren&lt;SkinnedMeshRenderer&gt;(); // 遍历SkinnedMeshRenderer for (int i = 0; i &lt; skinnedMeshRenderers.Length; i++) &#123; var skinMeshRenderer = skinnedMeshRenderers[i]; // 存储mesh的uv uv1List.Add(skinMeshRenderer.sharedMesh.uv); uv1Count += skinMeshRenderer.sharedMesh.uv.Length; uv2List.Add(skinMeshRenderer.sharedMesh.uv); uv2Count += skinMeshRenderer.sharedMesh.uv.Length; &#125;&#125; 2.4 合并完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122private void SkinnedMeshCombine(GameObject model)&#123; // 模型中所有的SkinnedMeshRenderer var skinnedMeshRenderers = model.GetComponentsInChildren&lt;SkinnedMeshRenderer&gt;(); var combineList = new List&lt;CombineInstance&gt;(); // 骨骼数据 var bones = new List&lt;Transform&gt;(); // uv var uvCount = 0; var uvList = new List&lt;Vector2[]&gt;(); // uv2的起始位置 var uv2StartIndex = 0; // 需要合并的贴图 var textureList = new List&lt;Texture2D&gt;(); Material material = null; // 骨骼 var allBones = model.GetComponentsInChildren&lt;Transform&gt;(); var ok = false; // 遍历SkinnedMeshRenderer for (int i = 0; i &lt; skinnedMeshRenderers.Length; i++) &#123; var skinMeshRenderer = skinnedMeshRenderers[i]; if (material == null) &#123; //返回分配给渲染器的第一个材质 material = skinMeshRenderer.sharedMaterial; &#125; // 需要合并的网格 var combine = new CombineInstance(); combine.mesh = skinMeshRenderer.sharedMesh; combine.transform = skinMeshRenderer.transform.localToWorldMatrix; combineList.Add(combine); // 贴图 var texture2d = mat.mainTexture as Texture2D; textureList.Add(texture2d); // 收集需要合并的骨骼信息 for(int j = 0; j &lt; skinMeshRenderer.bones.Length; j++) &#123; for(int k = 0; k &lt; allBones.Length; k++) &#123; if(smr.bones[j].name == allBones[k].name) &#123; bones.Add(allBones[k]); break; &#125; &#125; &#125; // 存储mesh的uv uvList.Add(skinMeshRenderer.sharedMesh.uv); uvCount += skinMeshRenderer.sharedMesh.uv.Length; if (skinMeshRenderer.name == &quot;&quot;) &#123; // 针对某个mesh使用了uv2 ok = true; uv2List.Add(render.sharedMesh.uv2); uv2Count += render.sharedMesh.uv2.Length; &#125; if (!ok) uv2StartIndex = uvCount; // 销毁子物体 Object.Destroy(skinMeshRenderer.gameObject); &#125; // 开始合并，创建一个新的节点 var combineObj = new GameObject(&quot;Combine&quot;); combineObj.transform.parent = model.transform; // 贴图合并 var skinnedMeshAtlas = new Texture2D(1024, 1024); var packingResults = skinnedMeshAtlas.PackTextures(textureList.ToArray(), 0); // 设置uv var atlasUVs = new Vector2[uvCount]; // 大Mesh的UV列表 var counter = 0; for (int index = 0; index &lt; uvList.Count; ++index) &#123; foreach (Vector2 uv in uvList[index]) &#123; atlasUVs[counter].x = Mathf.Lerp(packingResults[index].xMin, packingResults[index].xMax, uv.x); atlasUVs[counter].y = Mathf.Lerp(packingResults[index].yMin, packingResults[index].yMax, uv.y); counter++; &#125; &#125; // uv2采样 var meshAtlas2 = new Texture2D(512, 512); var packingResults2 = meshAtlas2.PackTextures(new Texture2D[] &#123; numberData.m_NumTexture &#125;, 0); var atlasUV2s = new Vector2[uvCount]; // 大Mesh的UV2列表 counter = uv2StartIndex; for (int index = 0; index &lt; uv2List.Count; ++index) &#123; foreach (Vector2 uv in uv2List[index]) &#123; atlasUV2s[counter].x = Mathf.Lerp(packingResults2[index].xMin, packingResults2[index].xMax, uv.x); atlasUV2s[counter].y = Mathf.Lerp(packingResults2[index].yMin, packingResults2[index].yMax, uv.y); counter++; &#125; &#125; var smr = combineObj.AddComponent&lt;SkinnedMeshRenderer&gt;(); smr.sharedMesh = new Mesh(); smr.sharedMesh.CombineMeshes(combineList.ToArray(), true, false); smr.sharedMesh.uv = atlasUVs; smr.sharedMesh.uv2 = atlasUV2s; smr.material = material; smr.material.mainTexture = skinnedMeshAtlas; smr.bones = bones.ToArray(); // smr.rootBone = transform;&#125; 原本模型下有多个SkinnedMeshRenderer，合并后只有一个了（旧的都被删除），虽然占用的内存变多了，但是原本渲染一个模型需要渲染头、四肢、身体、鞋子，和额外的装备，每部分都是不同的mesh，这样至少需要4个Draw Call（没有额外装备的情况），合并mesh和texture后，只需要一个Draw Call便可完整绘制，内存的付出是值得的。 2.5 另一种合并Texture的方式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051Texture2D Combine(Texture2D tex1, Texture2D tex2)&#123; int length = tex1.width * 2; var blcokBytes = 0; // 和不同的压缩算法相关 byte[] data = null; switch (tex1.format) &#123; case TextureFormat.DXT1: case TextureFormat.ETC_RGB4: case TextureFormat.ETC2_RGB: blcokBytes = 8; data = new byte[length / 2 * length / 2]; break; case TextureFormat.DXT5: case TextureFormat.ETC2_RGBA8: case TextureFormat.ASTC_4x4: blcokBytes = 16; data = new byte[length * length / 2]; break; default: UnityEngine.Debug.Log(&quot;Not supported.&quot;); return null; &#125; //填充第一张图到左边 CombineBlocks(tex1.GetRawTextureData(), data, 0, 0, tex1.width, 4, blcokBytes, length); //填充第二张图到右边 CombineBlocks(tex2.GetRawTextureData(), data, tex1.width, 0, tex2.width, 4, blcokBytes, length); // 合并后的宽高要与两张图合并的尺寸一致 // 这里简单把两张512 x 512的图合成一张大的1024 x 512 var combinedTex = new Texture2D(length, length / 2, tex1.format, false); combinedTex.LoadRawTextureData(data); combinedTex.Apply(false, true); return combinedTex;&#125;void CombineBlocks(byte[] src, byte[] dst, int dstx, int dsty, int width, int block, int bytes, int length)&#123; var dstbx = dstx / block; var dstby = dsty / block; for (int i = 0; i &lt; width / block; i++) &#123; int dstindex = (dstbx + (dstby + i) * (length / block)) * bytes; int srcindex = i * (width / block) * bytes; System.Buffer.BlockCopy(src, srcindex, dst, dstindex, width / block * bytes); &#125;&#125; 合并Texture应遵循： Texture压缩格式一致 Texture为正方形且分辨率一致 申请合并后的Texture数组应和两张图合并的byte数一致，取决于不同压缩算法 常见错误： 合并Texture时，新创建的数据集大于合并的数据集，报错:","categories":[{"name":"Uniy","slug":"Uniy","permalink":"https://raoyuqi.github.io/categories/Uniy/"},{"name":"性能优化","slug":"Uniy/性能优化","permalink":"https://raoyuqi.github.io/categories/Uniy/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"}],"tags":[{"name":"Uniy","slug":"Uniy","permalink":"https://raoyuqi.github.io/tags/Uniy/"},{"name":"性能优化","slug":"性能优化","permalink":"https://raoyuqi.github.io/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"合并Mesh","slug":"合并Mesh","permalink":"https://raoyuqi.github.io/tags/%E5%90%88%E5%B9%B6Mesh/"}]},{"title":"2D游戏相机跟随","slug":"unity/mix/2D游戏相机跟随","date":"2023-09-13T14:01:20.000Z","updated":"2023-09-13T14:03:04.953Z","comments":false,"path":"2023/09/13/unity/mix/2D游戏相机跟随/","link":"","permalink":"https://raoyuqi.github.io/2023/09/13/unity/mix/2D%E6%B8%B8%E6%88%8F%E7%9B%B8%E6%9C%BA%E8%B7%9F%E9%9A%8F/","excerpt":"","text":"1. 矩形地图1.1 思路 初始化相机视野边界 相机跟随移动时，将相机视野限制在视野边界内 1.2 实现 初始化地图边界 123456789101112131415161718192021222324// 地图public SpriteRenderer m_Map;// 用BoxCollider2D设置相机视野public BoxCollider2D m_BoxCollider2D;private float m_MinMapEdgeX = 0; // left edge of map in xprivate float m_MaxMapEdgeX = 0; // right edge of map in xprivate float m_MinMapEdgeY = 0; // bottom edge of map in yprivate float m_MaxMapEdgeY = 0; // top edge of map in yprivate void InitMapBorder()&#123; if (this.m_BoxCollider2D == null || this.m_Map == null) return; // 将实际边界减去相机size的宽高，计算出相机中心坐标的移动范围 var mapExtents = this.m_Map.bounds.extents; var cameraExtents = this.m_BoxCollider2D.bounds.extents; this.m_MaxMapEdgeX = mapExtents.x - cameraExtents.x; this.m_MinMapEdgeX = -this.m_MaxMapEdgeX; this.m_MaxMapEdgeY = mapExtents.y - cameraExtents.y; this.m_MinMapEdgeY = -this.m_MaxMapEdgeY;&#125; 相机跟随 123456789101112131415161718192021222324252627282930public Transform m_FollowTarget;private void FixedUpdate()&#123; if (this.m_FollowTarget != null) &#123; var targetPosition = this.m_FollowTarget.position + this.m_Offset; targetPosition = this.GetLimitPosition(targetPosition); this.transform.position = targetPosition; &#125;&#125;// 坐标限制private Vector3 GetLimitPosition(Vector3 pos)&#123; if (pos.x &gt; this.m_MaxMapEdgeX) pos.x = this.m_MaxMapEdgeX; if (pos.x &lt; this.m_MinMapEdgeX) pos.x = this.m_MinMapEdgeX; if (pos.y &gt; this.m_MaxMapEdgeY) pos.y = this.m_MaxMapEdgeY; if (pos.y &lt; this.m_MinMapEdgeY) pos.y = this.m_MinMapEdgeY; return pos;&#125; &lt;/br&gt; 2. 多边形地图2.1 思路 移动实时计算相机的视野边界坐标 判断视野的上下左右边界是否在多边形内 2.2 实现 计算相机视野边界 123456public Camera m_Camera;Vector2 minCameraEdgeX = this.m_Camera.ViewportToWorldPoint(new Vector3(0, 0.5f, this.m_Camera.nearClipPlane)); // left edge of camera in xVector2 maxCameraEdgeX = this.m_Camera.ViewportToWorldPoint(new Vector3(1, 0.5f, this.m_Camera.nearClipPlane)); // right edge of camera in xVector2 minCameraEdgeY = this.m_Camera.ViewportToWorldPoint(new Vector3(0.5f, 0, this.m_Camera.nearClipPlane)); // bottom edge of camera in yVector2 maxCameraEdgeY = this.m_Camera.ViewportToWorldPoint(new Vector3(0.5f, 1, this.m_Camera.nearClipPlane)); // top edge of camera in y 判断点是否在多边形内算法 1234567891011121314151617181920212223242526272829303132/// ray-crossing算法，两点式方程公式：(y-y1)/(y2-y1)=(x-x1)/(x2-x1) 求位于直线上的点(x, y)/// 判断点是否在多边形内./// 注意到如果从P作水平向右的射线的话，如果P在多边形内部，那么这条射线与多边形的交点必为奇数，/// 如果P在多边形外部，则交点个数必为偶数(0也在内)。public static bool PointIsInPolygon(this Vector2 point, Vector2[] polygon)&#123; var count = 0; var len = polygon.Length; var point1 = Vector2.zero; var point2 = Vector2.zero; for (int i = 0; i &lt; len; i++) &#123; point1 = polygon[i]; point2 = polygon[(i + 1) % len]; // 水平边，跳过 if (point1.y == point2.y) continue; // 点不在线段两端点范围内 if (point.y &lt; Mathf.Min(point1.y, point2.y) | point.y &gt; Mathf.Max(point1.y, point2.y)) continue; // 经过点point，往右画一条直线，统计交点count次数 var x = (point.y - point1.y) * (double)(point2.x - point1.x) / (double)(point2.y - point1.y) + point1.x; if (x &gt; point.x) count++; &#125; // 相交奇数次说明在多边形内 return count % 2 == 1;&#125; 完整代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677using UnityEngine;// 位标志public enum OutBoundType&#123; None = 0, Left = 0x0001, Right = 0x0002, Up = 0x0004, Bottom = 0x0008&#125;public class CameraFollowForPolygonMap : MonoBehaviour&#123; public Transform m_FollowTarget; // 地图形状 public Collider2D m_BoudingShape2D; public Camera m_Camera; private Vector3 m_Offset; private Bounds m_Bounds; private void Start() &#123; this.m_Offset = this.transform.position - this.m_FollowTarget.position; this.m_Bounds = this.m_BoudingShape2D.composite.bounds.size == Vector3.zero ? this.m_BoudingShape2D.bounds : this.m_BoudingShape2D.composite.bounds; &#125; private void FixedUpdate() &#123; if (this.m_FollowTarget == null) return; var targetPosition = this.m_FollowTarget.position + this.m_Offset; var lastPosition = this.transform.position; var offset = targetPosition - lastPosition; var boundType = GetCameraOutBoundTypeAfterMoving(offset); if (boundType.HasFlag(OutBoundType.Left) || boundType.HasFlag(OutBoundType.Right)) targetPosition.x = lastPosition.x; if (boundType.HasFlag(OutBoundType.Bottom) || boundType.HasFlag(OutBoundType.Up)) targetPosition.y = lastPosition.y; this.transform.position = targetPosition; &#125; /// 获取相机移动后的越界类型, None: 未出界 private OutBoundType GetCameraOutBoundTypeAfterMoving(Vector2 offset) &#123; Vector2 minCameraEdgeX = this.m_Camera.ViewportToWorldPoint(new Vector3(0, 0.5f, this.m_Camera.nearClipPlane)); // left edge of camera in x Vector2 maxCameraEdgeX = this.m_Camera.ViewportToWorldPoint(new Vector3(1, 0.5f, this.m_Camera.nearClipPlane)); // right edge of camera in x Vector2 minCameraEdgeY = this.m_Camera.ViewportToWorldPoint(new Vector3(0.5f, 0, this.m_Camera.nearClipPlane)); // bottom edge of camera in y Vector2 maxCameraEdgeY = this.m_Camera.ViewportToWorldPoint(new Vector3(0.5f, 1, this.m_Camera.nearClipPlane)); // top edge of camera in y // 这里用的是Unity自带接口判断 var ret = OutBoundType.None; // 左边越界 if (!this.m_Bounds.Contains(minCameraEdgeX + offset)) ret = ret | OutBoundType.Left; // 右边出界 if (!this.m_Bounds.Contains(maxCameraEdgeX + offset)) ret = ret | OutBoundType.Right; // 上边出界 if (!this.m_Bounds.Contains(maxCameraEdgeY + offset)) ret = ret | OutBoundType.Up; // 下边出界 if (!this.m_Bounds.Contains(minCameraEdgeY + offset)) ret = ret | OutBoundType.Bottom; return ret; &#125;&#125;","categories":[{"name":"Uniy","slug":"Uniy","permalink":"https://raoyuqi.github.io/categories/Uniy/"}],"tags":[{"name":"Uniy","slug":"Uniy","permalink":"https://raoyuqi.github.io/tags/Uniy/"},{"name":"Dots","slug":"Dots","permalink":"https://raoyuqi.github.io/tags/Dots/"}]},{"title":"JobSystem","slug":"unity/JobSystem","date":"2023-08-10T13:26:46.000Z","updated":"2023-08-10T13:42:23.482Z","comments":false,"path":"2023/08/10/unity/JobSystem/","link":"","permalink":"https://raoyuqi.github.io/2023/08/10/unity/JobSystem/","excerpt":"","text":"1. 概述Unity的Job System可以很方便的编写多线程代码，多线程可以充分发挥多核的优势。可以单独使用，但为了提高性能，通常结合Burst编译器一起使用，这是专门为Unity的Job System编译作业而设计的。Burst编译器改进了代码生成，从而提高了性能并减少了移动设备上的电池消耗。也可以用在ECS上，实现高性能的面向数据代码。 Job System的调度策略使用了工作窃取策略，以均匀分配跨工作线程共享的任务数量。思想是：成了所有任务的工作线程会查看其它工作线程的任务队列，然后将任务分配给另一个工作线程。 2. Job Types IJob：在job thread运行的单个任务 IJobFor：与IJobParallelFor相同，但允许调度Job，使其不会并行运行 IJobParallelFor：并行运行任务。每个并行运行的工作线程都有一个独占索引，以便安全地访问工作线程之间的共享数据 2.1 IJob定义一个Job： 1234567891011public struct AddJob : IJob&#123; public float a; public float b; public NativeArray&lt;float&gt; result; public void Execute() &#123; result[0] = a + b; &#125;&#125; Job中的变量仅可以使用blittable types或者Unity提供的NativeContainer容器，比如引擎内置的NativeArray或者com.unity.collections package中提供的容器类。 为什么只能使用blittable types？ 为了资源竞争问题，Job System向每个任务发送它需要操作的数据的副本，而不是主线程中对数据的引用。该副本隔离了数据，从而消除了竞争条件。blittable types在这个拷贝过程中不需要做数据转换，因此blittable types在这里是必须的。 Job调度： 1234567891011121314void Start()&#123; var job = new AddJob() &#123; a = 1, b = 2, result = this._result &#125;; // 调度 this._handler = job.Schedule(); // 调用Complete获取结果 this._handler.Complete(); Debug.Log($&quot;Start _result = &#123;this._result[0]&#125;&quot;);&#125; 完整代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152using Unity.Collections;using Unity.Jobs;using UnityEngine;public struct AddJob : IJob&#123; public float a; public float b; public NativeArray&lt;float&gt; result; public void Execute() &#123; result[0] = a + b; &#125;&#125;public class AddJob : MonoBehaviour&#123; private NativeArray&lt;float&gt; _result; private JobHandle _handler; void Start() &#123; this._result = new NativeArray&lt;float&gt;(1, Allocator.Persistent); var job = new AddJob() &#123; a = 1, b = 2, result = this._result &#125;; this._handler = job.Schedule(); this._handler.Complete(); Debug.Log($&quot;Start _result = &#123;this._result[0]&#125;&quot;); &#125; // void Update() // &#123; // if (this._handler.IsCompleted) // &#123; // this._handler.Complete(); // Debug.Log($&quot;Update _result = &#123;this._result[0]&#125;&quot;); // &#125; // &#125; void OnDestroy() &#123; // 释放非托管资源 if (this._result.IsCreated) this._result.Dispose(); &#125;&#125; 2.2 IJobFor与对比IJobFor： 123456789public interface IJobFor&#123; void Execute(int index);&#125;public interface IJob&#123; void Execute();&#125; IJobFor接口的Execute方法多了一个index参数，通过这个参数可以访问Job中的NativeContainer容器，对容器中的元素进行相对独立的操作。除此之外，IJobFor还在任务的调度上提供了更大的灵活性。可以用下面三种方式调度Job： 1234567891011121314public void Update()&#123; ... var position = new NativeArray&lt;Vector3&gt;(500, Allocator.Persistent); var job = new VelocityJob(); // run on main thread job.Run(position.Length); // run on a single worker thread job.Schedule(position.Length, dependency); // run on parallel worker threads job.ScheduleParallel(position.Length, 64, dependency); ...&#125; 以上三种方式都需要传入一个arrayLength参数，这个参数可以控制Execute方法执行的次数。 实际上传入的arrayLength不一定就是数组的长度，它可以是小于数组长度的任意数值。Unity官方例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354using Unity.Collections;using Unity.Jobs;using UnityEngine;public class ApplyVelocityParallelForSample : MonoBehaviour&#123; struct VelocityJob : IJobFor &#123; // By declaring it as read only, multiple jobs are allowed to access the data in parallel [ReadOnly] public NativeArray&lt;Vector3&gt; velocity; // By default containers are assumed to be read &amp; write public NativeArray&lt;Vector3&gt; position; public float deltaTime; public void Execute(int i) &#123; position[i] = position[i] + velocity[i] * deltaTime; &#125; &#125; void Update() &#123; var position = new NativeArray&lt;Vector3&gt;(500, Allocator.Persistent); var velocity = new NativeArray&lt;Vector3&gt;(500, Allocator.Persistent); for (var i = 0; i &lt; velocity.Length; i++) velocity[i] = new Vector3(0, 10, 0); var job = new VelocityJob() &#123; deltaTime = Time.deltaTime, position = position, velocity = velocity &#125;; // run on main thread job.Run(position.Length); //run on a single worker thread var sheduleJobDependency = new JobHandle(); var sheduleJobHandle = job.Schedule(position.Length, sheduleJobDependency); //run on parallel worker threads var sheduleParralelJobHandle = job.ScheduleParallel(position.Length, 64, sheduleJobHandle); sheduleParralelJobHandle.Complete(); Debug.Log(job.position[0]); position.Dispose(); velocity.Dispose(); &#125;&#125; 把velocity标记为ReadOnly时，可以在多个并行的Job中读取velocity数组的内容而不触发safety system。因此应该尽量将Job中只读的数据标记成ReadOnly来最大化性能。 IJobFor的三种不同用法在性能上的表现截图： Job.Run：行在主线程的 Job.Schedule：由于任务窃取的调度策略Job.Schedule也在主线程，当主线程在等待工作（worker）线程执行的过程中也会从任务池中获取任务来执行，从而加快所有任务的完成进度 Job.ScheduleParallel：时间明显要短于前两个，通过ScheduleParallel把任务分发到各个工作线程，让每个线程负责一部分工作 应该尽量使用IJobFor.ScheduleParallel来把任务拆分到多个核心去做并行计算，才能最大化程序的执行效率。 3. 线程本地资源应用场景：每次随机一个方向进行移动： 12345678910111213141516171819202122232425struct RandomVelocityJob : IJobFor&#123; [ReadOnly] public NativeArray&lt;float&gt; speeds; [ReadOnly] public Random random; public float deltaTime; // output public NativeArray&lt;float3&gt; positions; public void Execute(int i) &#123; positions[i] += random.NextFloat3Direction() * speeds[i] * deltaTime; &#125;&#125;var random = new Random(1234);var randomVelocityJob = new RandomVelocityJob&#123; speeds = m_Speeds, random = random, positions = m_Positions, deltaTime = Time.deltaTime,&#125;; NextFloat3Direction方法会改变random的内部状态，由于所有的worker线程会共享并改变random的状态，使random处在竞争条件的状态。加锁是效率比较低的方法，另外一种比较经典的做法是使用线程本地存储（TLS），让每个线程拥有一份自己独有的资源，避免了竞争条件的问题。 使用JobsUtility.MaxJobThreadCount来获取worker的最大数量，为每一个worker线程初始化一个random变量： 1234567private NativeArray&lt;Random&gt; _randoms;this._randoms = new NativeArray&lt;Random&gt;(JobsUtility.MaxJobThreadCount, Allocator.Persistent);for (int i = 0; i &lt; m_Randoms.Length; i++)&#123; _randoms[i] = Random.CreateFromIndex((uint)i);&#125; 在Job中需要知道当前执行的线程ID，可以在Job中声明一个int类型的变量并添加[NativeSetThreadIndex]属性，在job执行的过程中Unity会自动注入Job ID。 这样就可以在Execute方法中利用线程ID获取线程独有的资源了： 123456789101112131415161718192021struct RandomVelocityJob : IJobFor&#123; // input [ReadOnly] public NativeArray&lt;float&gt; speeds; [ReadOnly] public NativeArray&lt;Random&gt; randoms; public float deltaTime; // 自动注入线程Id [NativeSetThreadIndex] private int threadIdx; // output public NativeArray&lt;float3&gt; positions; public void Execute(int i) &#123; positions[i] += randoms[threadIdx].NextFloat3Direction() * speeds[i] * deltaTime; &#125;&#125; 4. Pointers &amp; InterLocked利用IJobFor接口开实现并行求和： 12345678910public struct NaiveParallelCounterJob : IJobFor&#123; [ReadOnly] public NativeArray&lt;int&gt; data; public int sum; public void Execute(int index) &#123; sum += data[index]; &#125;&#125; 这断代码会导致任务完成之后无法获取正确的sum值，因为sum不是以引用的形式传进来的而是直接以值传递的形式进行了赋值，修改： 1234567891011public struct NaiveParallelCounterJob : IJobFor&#123; [ReadOnly] public NativeArray&lt;int&gt; data; // 允许以引用的方式来传递单个值 public NativeReference&lt;int&gt; naiveSum; public void Execute(int index) &#123; naiveSum.Value += data[index]; &#125;&#125; Job在多线程中运行时，对naiveSum变量的访问可能会存在竞争条件，使用TLS解决： 123456789101112131415161718192021222324252627282930313233343536public struct ThreadLocalParallelCounterJob : IJobFor&#123; //input [NativeDisableParallelForRestriction] public NativeArray&lt;int&gt; data; //output [NativeDisableParallelForRestriction] public NativeArray&lt;int&gt; sums; [NativeSetThreadIndex] private int _threadIndex; public void Execute(int index) &#123; sums[_threadIndex] += data[index]; &#125;&#125;public struct TotalSumJob : IJob&#123; [ReadOnly] public NativeArray&lt;int&gt; sums; public NativeReference&lt;int&gt; totalSum; public void Execute() &#123; for (int i = 0; i &lt; sums.Length; i++) &#123; totalSum.Value += sums[i]; &#125; &#125;&#125;// Job依赖var threadLocalCounterJobHandle = threadLocalCounterJob.ScheduleParallel(m_Data.Length, 64, new JobHandle());var totalSumJobHandle = totalSumJob.Schedule(threadLocalCounterJobHandle);totalSumJobHandle.Complete(); 使用Interlocked实现： 12345678910111213public unsafe struct InterlockedParallelCounterJob : IJobFor&#123; //input public NativeArray&lt;int&gt; data; //output [NativeDisableUnsafePtrRestriction] public int* sum; public void Execute(int index) &#123; Interlocked.Add(ref UnsafeUtility.AsRef&lt;int&gt;(sum), data[index]); &#125;&#125; Interlocked可以在不同线程之间以原子操作的方式来修改变量，这样就不用担心竞争条件的问题了。为了在Job中使用指针，需要引入一个新的属性——[NativeDisableUnsafePtrRestriction]。 完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125using System.Threading;using Unity.Collections;using Unity.Collections.LowLevel.Unsafe;using Unity.Jobs;using Unity.Jobs.LowLevel.Unsafe;using UnityEngine;public unsafe class ParallelCounter : MonoBehaviour&#123; public struct ThreadLocalParallelCounterJob : IJobFor &#123; //input [NativeDisableParallelForRestriction] public NativeArray&lt;int&gt; data; //output [NativeDisableParallelForRestriction] public NativeArray&lt;int&gt; sums; [NativeSetThreadIndex] private int m_ThreadIndex; public void Execute(int index) &#123; sums[m_ThreadIndex] += data[index]; &#125; &#125; public struct TotalSumJob : IJob &#123; [ReadOnly] public NativeArray&lt;int&gt; sums; public NativeArray&lt;int&gt; totalSum; public void Execute() &#123; for (int i = 0; i &lt; sums.Length; i++) &#123; totalSum[0] += sums[i]; &#125; &#125; &#125; public unsafe struct InterlockedParallelCounterJob : IJobFor &#123; //input public NativeArray&lt;int&gt; data; //output [NativeDisableUnsafePtrRestriction] public int* sum; public void Execute(int index) &#123; Interlocked.Add(ref UnsafeUtility.AsRef&lt;int&gt;(sum), data[index]); &#125; &#125; private static readonly int DATA_COUNT = 100000; private NativeArray&lt;int&gt; _data; private NativeArray&lt;int&gt; _threadLocalSums; void Start() &#123; this._data = new NativeArray&lt;int&gt;(DATA_COUNT, Allocator.Persistent); for (int i = 0; i &lt; DATA_COUNT; i++) this._data[i] = i; this._threadLocalSums = new NativeArray&lt;int&gt;(JobsUtility.MaxJobThreadCount, Allocator.Persistent); ResetThreadLocalSums(); &#125; private void ResetThreadLocalSums() &#123; for (int i = 0; i &lt; this._threadLocalSums.Length; i++) &#123; this._threadLocalSums[i] = 0; &#125; &#125; private void OnDestroy() &#123; this._data.Dispose(); this._threadLocalSums.Dispose(); &#125; void Update() &#123; var threadLocalCounterJob = new ThreadLocalParallelCounterJob &#123; data = this._data, sums = this._threadLocalSums &#125;; var totalSum = new NativeArray&lt;int&gt;(1, Allocator.TempJob); var totalSumJob = new TotalSumJob &#123; sums = this._threadLocalSums, totalSum = totalSum &#125;; var threadLocalCounterJobHandle = threadLocalCounterJob.ScheduleParallel(this._data.Length, 64, new JobHandle()); var totalSumJobHandle = totalSumJob.Schedule(threadLocalCounterJobHandle); totalSumJobHandle.Complete(); ResetThreadLocalSums(); Debug.Log($&quot;[ThreadLocalParallelCounter] Sum = &#123;totalSum[0]&#125;&quot;); totalSum.Dispose(); var sum = (int*)UnsafeUtility.Malloc(UnsafeUtility.SizeOf&lt;int&gt;(), UnsafeUtility.AlignOf&lt;int&gt;(), Allocator.TempJob); *sum = 0; var interlockedParallelCounterJob = new InterlockedParallelCounterJob &#123; data = this._data, sum = sum &#125;; var interlockedCounterJobHandle = interlockedParallelCounterJob.ScheduleParallel(this._data.Length, 64, new JobHandle()); interlockedCounterJobHandle.Complete(); Debug.Log($&quot;[InterlockedParallelCounterJob] Sum = &#123;*sum&#125;&quot;); UnsafeUtility.Free(sum, Allocator.TempJob); &#125;&#125; 5. innerloopBatchCount官方说明：innerloopBatchCount是执行任务窃取的粒度。并在这个worker线程中连续调用innerloopBatchCount次的Execute(index)方法。当每次迭代中都有大量工作时，将值设为1是合理的。反之设置为32或64是合理的。 测试不同batchCount对性能的影响： 12345678910111213141516171819202122232425262728293031var job = new VelocityJob()&#123; deltaTime = Time.deltaTime, position = _positions, velocity = _velocity&#125;;var batchCount = 1;Profiler.BeginSample($&quot;Batch = &#123;batchCount&#125;&quot;);job.ScheduleParallel(_positions.Length, batchCount, new JobHandle()).Complete();Profiler.EndSample();batchCount = 8;Profiler.BeginSample($&quot;Batch = &#123;batchCount&#125;&quot;);job.ScheduleParallel(_positions.Length, batchCount, new JobHandle()).Complete();Profiler.EndSample();batchCount = 16;Profiler.BeginSample($&quot;Batch = &#123;batchCount&#125;&quot;);job.ScheduleParallel(_positions.Length, batchCount, new JobHandle()).Complete();Profiler.EndSample();batchCount = 32;Profiler.BeginSample($&quot;Batch = &#123;batchCount&#125;&quot;);job.ScheduleParallel(m_Positions.Length, batchCount, new JobHandle()).Complete();Profiler.EndSample();batchCount = 64;Profiler.BeginSample($&quot;Batch = &#123;batchCount&#125;&quot;);job.ScheduleParallel(_positions.Length, batchCount, new JobHandle()).Complete();Profiler.EndSample(); 如图： Batch=1时性能最差，Batch=8时性能稍好，当Batch为8的整数倍时，性能是相近的。出现这个现象的原因是伪共享。CPU缓存结构： 一个 CPU 里通常会有多个 CPU 核心，如上图中的 CPU1 和 CPU2，并且每个 CPU 核心都有自己的 L1 Cache 和 L2 Cache，而 L1 Cache 通常分为 DCache（数据缓存） 和 ICache（指令缓存），L3 Cache 则是多个核心共享的。 伪共享产生的原因： 由于缓存读取的机制，不是一次只读一个数据，而是一次读取一个CacheLine的数据。如果有两个CPU同时访问到内存中的同一个CachLine数据时，CacheLine会同时被加载进入CPU1和CPU2中。由于MESI协议，CPU1修改数据时，CPU2所共享的CacheLine将会失效，CPU2只能等到CPU1修改完后，再重新获取CacheLine并修改。这样就导致了CPU2运行效率的降低。这个问题就是false sharing。 Batch=1便是伪共享导致效率低下，分析Batch数为8的整数倍时运行效率几乎一样的原因： 123456789101112struct VelocityJob : IJobFor&#123; [ReadOnly] public NativeArray&lt;float3&gt; velocity; public NativeArray&lt;float3&gt; position; public float deltaTime; public void Execute(int i) &#123; position[i] += velocity[i] * deltaTime; &#125;&#125; velocity和position都是float3类型，大小是4 3 = 12字节，电脑的cache line大小是32字节，得出 12 8 == 32 * 3 == 96，因此当batch大小是8的倍数时恰好可以避免伪共享（false sharing）问题，因此时间几乎一致。 6. SoA vs AoSarray of structures (AoS)和structure of arrays (SoA)代表两种不同的数据组织形式： 123456789101112131415struct AoSData&#123; public int a; public int b; public int c; public int d;&#125;struct SoAData&#123; public NativeArray&lt;int&gt; aArray; public NativeArray&lt;int&gt; bArray; public NativeArray&lt;int&gt; cArray; public NativeArray&lt;int&gt; dArray;&#125; 内存布局： AoSData的数据a，b，c，d在内存中是交错存在的。而SoAData中相同的数据在内存中是连续存在的。两种形式对于数据访问的性能： 假设a，b，c，d四个数据的大小是一样的并且当前CPU的cache line一次可以加载4个数据即一个cache line可以加载a，b，c，d或者a，a，a，b四个数据。使用AoSData的情况下访问所有的a数据需要访问3次内存： 加载abcd，找到第一个a 加载abcd，找到第二个a 加载abcd，找到第三个a 使用SoAData只需要一次，加载aaab，找到三个a。且SoA对SIMD指令更加友好，Burst对SoA形式的数据也有特别的支持，都能让SoAData具有更好的性能，编码测试。 数据定义： 1234567891011121314// Unity中典型的AoS和SoA数据的组织形式public struct TransformAoS&#123; public float3 position; public quaternion rotation; public float3 scale;&#125;public class TransformSoA&#123; public NativeArray&lt;float3&gt; positions; public NativeArray&lt;quaternion&gt; rotations; public NativeArray&lt;float3&gt; scales;&#125; AoS的Job实现： 12345678910111213public struct TransformAoSJob : IJobFor&#123; public NativeArray&lt;TransformAoS&gt; transformAoSes; [ReadOnly] public float3 velocity; [ReadOnly] public float deltaTime; public void Execute(int index) &#123; var transAoS = transformAoSes[index]; transAoS.position += velocity * deltaTime; transformAoSes[index] = transAoS; &#125;&#125; var transAoS = transformAoSes[index]会产生一次结构体拷贝，结构体比较小的时候性能开销并不高，但是当结构体中数据量比较大的时候，开销也会升高。由于transAoS是一个拷贝副本，因此对它的修改不会反映在数据本体上，所以在Job的最后把拷贝出来的值又赋值回了transformAoSes数组，又产生了一次数据拷贝。利用指针来获取原始数据的引用改进代码： 1234567891011121314151617public struct TransformAoSJob : IJobFor&#123; public NativeArray&lt;TransformAoS&gt; transformAoSes; [ReadOnly] public float3 velocity; [ReadOnly] public float deltaTime; public void Execute(int index) &#123; // var transAoS = transformAoSes[index]; // transAoS.position += velocity * deltaTime; // transformAoSes[index] = transAoS; var transformPtr = (TransformAoS*)transformAoSes.GetUnsafePtr(); ref var transform = ref transformPtr[index]; transform.position += velocity * deltaTime; &#125;&#125; SoA的Job实现： 1234567891011public struct TransformSoAJob : IJobFor&#123; [NoAlias] public NativeArray&lt;float3&gt; positions; [ReadOnly] public float3 velocity; [ReadOnly] public float deltaTime; public void Execute(int index) &#123; positions[index] += velocity * deltaTime; &#125;&#125; 需要使用Burst编译，否则无差别，没使用BurstCompile的性能对比： BurstCompile性能对比： 完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135using System;using Unity.Burst;using Unity.Collections;using Unity.Collections.LowLevel.Unsafe;using Unity.Jobs;using Unity.Mathematics;using UnityEngine;using Random = Unity.Mathematics.Random;namespace JobSystem.SoA_VS_AoS&#123; public struct TransformAoS &#123; public float3 position; public quaternion rotation; public float3 scale; &#125; public unsafe class TransformBehaviour : MonoBehaviour &#123; public class TransformSoA : IDisposable &#123; public NativeArray&lt;float3&gt; positions; public NativeArray&lt;quaternion&gt; rotations; public NativeArray&lt;float3&gt; scales; public TransformSoA(int count) &#123; Create(count); &#125; private void Create(int count) &#123; positions = new NativeArray&lt;float3&gt;(count, Allocator.Persistent); rotations = new NativeArray&lt;quaternion&gt;(count, Allocator.Persistent); scales = new NativeArray&lt;float3&gt;(count, Allocator.Persistent); &#125; public void Dispose() &#123; if (positions.IsCreated) positions.Dispose(); if (rotations.IsCreated) rotations.Dispose(); if (scales.IsCreated) scales.Dispose(); &#125; &#125; [BurstCompile] public struct TransformAoSJob : IJobFor &#123; public NativeArray&lt;TransformAoS&gt; transformAoSes; [ReadOnly] public float3 velocity; [ReadOnly] public float deltaTime; public void Execute(int index) &#123; var transformPtr = (TransformAoS*)transformAoSes.GetUnsafePtr(); ref var transform = ref transformPtr[index]; transform.position += velocity * deltaTime; &#125; &#125; [BurstCompile] public struct TransformSoAJob : IJobFor &#123; public NativeArray&lt;float3&gt; positions; [ReadOnly] public float3 velocity; [ReadOnly] public float deltaTime; public void Execute(int index) &#123; var positionPtr = (float3*)positions.GetUnsafePtr(); ref var position = ref positionPtr[index]; position += velocity * deltaTime; &#125; &#125; private NativeArray&lt;TransformAoS&gt; m_TransformAoSes; private TransformSoA m_TransformSoA; private float3 m_Velocity; private static readonly int TRANSFORM_COUNT = 5000000; private void Start() &#123; m_TransformAoSes = new NativeArray&lt;TransformAoS&gt;(TRANSFORM_COUNT, Allocator.Persistent); m_TransformSoA = new TransformSoA(TRANSFORM_COUNT); var transformAoSPtr = (TransformAoS*)m_TransformAoSes.GetUnsafePtr(); var rand = new Random(1332); m_Velocity = rand.NextFloat3Direction(); for (int i = 0; i &lt; TRANSFORM_COUNT; i++) &#123; ref var transAoS = ref transformAoSPtr[i]; transAoS.position = rand.NextFloat3(); transAoS.rotation = rand.NextQuaternionRotation(); transAoS.scale = new float3(1, 1, 1); m_TransformSoA.positions[i] = rand.NextFloat3(); m_TransformSoA.rotations[i] = rand.NextQuaternionRotation(); m_TransformSoA.scales[i] = new float3(1, 1, 1); &#125; &#125; private void OnDestroy() &#123; m_TransformAoSes.Dispose(); m_TransformSoA.Dispose(); &#125; private void Update() &#123; new TransformSoAJob &#123; positions = m_TransformSoA.positions, velocity = m_Velocity, deltaTime = Time.deltaTime &#125;.ScheduleParallel(m_TransformSoA.positions.Length, 64, new JobHandle()).Complete(); new TransformAoSJob &#123; transformAoSes = m_TransformAoSes, velocity = m_Velocity, deltaTime = Time.deltaTime &#125;.ScheduleParallel(m_TransformAoSes.Length, 64, new JobHandle()).Complete(); &#125; &#125;&#125;","categories":[{"name":"Uniy","slug":"Uniy","permalink":"https://raoyuqi.github.io/categories/Uniy/"}],"tags":[{"name":"Uniy","slug":"Uniy","permalink":"https://raoyuqi.github.io/tags/Uniy/"},{"name":"Dots","slug":"Dots","permalink":"https://raoyuqi.github.io/tags/Dots/"}]},{"title":"颜色","slug":"graphic/opengl/颜色","date":"2023-08-07T13:20:14.000Z","updated":"2023-08-07T13:51:31.187Z","comments":false,"path":"2023/08/07/graphic/opengl/颜色/","link":"","permalink":"https://raoyuqi.github.io/2023/08/07/graphic/opengl/%E9%A2%9C%E8%89%B2/","excerpt":"","text":"1. 概述颜色空间有RGB、CMYK、LAB、HSV等，其中RGB是游戏中最常用的。它由红色(Red)、绿色(Green)和蓝色(Blue)三个分量组成，通常每个分量用8位存储，则rgb可表示2的24次方种颜色。用这三个值就可以组合出任意一种颜色，例如需要白色，可以定义如下颜色向量： 1glm::vec3 white(1.0f, 1.0f, 1.0f); 现实生活中能看到物体的颜色并不是物体真的由颜色，而是物体表面无法吸收而反射的颜色。太阳光是由多种不同颜色的光组成的白色，照射到红色物体后，它会吸收除了红色以外的所有颜色，不能吸收的将被反射，最后进入人眼，因此看到的都是物体反射的颜色，如下： 图形渲染中反射颜色： 123456// 白光glm::vec3 lightColor(1.0f, 1.0f, 1.0f);// 物体颜色反射系数glm::vec3 toyColor(1.0f, 0.0f, 0.0f);// 结果(1.0f, 0.0f, 0.0f);glm::vec3 result = lightColor * toyColor; 物体根据自身的颜色值对红、绿、蓝三个分量都做出了一定的反射，也表现了现实中颜色的工作原理。由此，可以定义物体的颜色为物体从一个光源反射各个颜色分量的大小。改变光照颜色也可以改变物体的颜色： 123456// 绿光glm::vec3 lightColor(0.0f, 1.0f, 0.0f);// 物体颜色反射系数glm::vec3 toyColor(1.0f, 0.0f, 0.0f);// 结果(0.0f, 0.0f, 0.0f);glm::vec3 result = lightColor * toyColor; 2. 创建光照场景光源的顶点着色器： 1234567891011#version 330 corelayout (location = 0) in vec3 aPos;uniform mat4 model;uniform mat4 view;uniform mat4 projection;void main()&#123; gl_Position = projection * view * model * vec4(aPos, 1.0);&#125; 光源的片段着色器，让它一直呈现白色： 12345678#version 330 coreout vec4 FragColor;void main()&#123; // 将向量的四个分量全部设置为1.0 FragColor = vec4(1.0);&#125; 物体的顶点着色器： 1234567891011#version 330 corelayout (location = 0) in vec3 aPos;uniform mat4 model;uniform mat4 view;uniform mat4 projection;void main()&#123; gl_Position = projection * view * model * vec4(aPos, 1.0);&#125; 物体的片段着色器： 12345678910#version 330 coreout vec4 FragColor;uniform vec3 objectColor;uniform vec3 lightColor;void main()&#123; FragColor = vec4(lightColor * objectColor, 1.0);&#125; 渲染光源： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// 光源位置glm::vec3 lightPos(1.2f, 1.0f, 2.0f);...// 物体unsigned int VBO, cubeVAO;glGenVertexArrays(1, &amp;cubeVAO);glGenBuffers(1, &amp;VBO);glBindBuffer(GL_ARRAY_BUFFER, VBO);glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);glBindVertexArray(cubeVAO);glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0);glEnableVertexAttribArray(0);// 光源unsigned int lightCubeVAO;glGenVertexArrays(1, &amp;lightCubeVAO);glBindVertexArray(lightCubeVAO);glBindBuffer(GL_ARRAY_BUFFER, VBO);glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0);glEnableVertexAttribArray(0);...// 渲染物体cubeShader.use();cubeShader.setVec3(&quot;objectColor&quot;, 1.0f, 0.5f, 0.31f);cubeShader.setVec3(&quot;lightColor&quot;, 1.0f, 1.0f, 1.0f);glm::mat4 projection = glm::perspective(glm::radians(camera.Zoom), (float)SCR_WIDTH / (float)SCR_HEIGHT, 0.1f, 100.0f);glm::mat4 view = camera.GetViewMatrix();cubeShader.setMat4(&quot;projection&quot;, projection);cubeShader.setMat4(&quot;view&quot;, view);glm::mat4 model = glm::mat4(1.0f);cubeShader.setMat4(&quot;model&quot;, model);glBindVertexArray(cubeVAO);glDrawArrays(GL_TRIANGLES, 0, 36);// 渲染光源lightShader.use();cubeShader.setMat4(&quot;projection&quot;, projection);cubeShader.setMat4(&quot;view&quot;, view);model = glm::mat4(1.0f);model = glm::translate(model, lightPos);model = glm::scale(model, glm::vec3(0.2f));cubeShader.setMat4(&quot;model&quot;, model);glBindVertexArray(lightCubeVAO);glDrawArrays(GL_TRIANGLES, 0, 36); 效果：","categories":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"}],"tags":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"OpenGL","slug":"OpenGL","permalink":"https://raoyuqi.github.io/tags/OpenGL/"}]},{"title":"摄像机","slug":"graphic/opengl/摄像机","date":"2023-07-29T02:12:24.000Z","updated":"2023-07-29T03:31:34.544Z","comments":false,"path":"2023/07/29/graphic/opengl/摄像机/","link":"","permalink":"https://raoyuqi.github.io/2023/07/29/graphic/opengl/%E6%91%84%E5%83%8F%E6%9C%BA/","excerpt":"","text":"1. 构建摄像机1.1 确定位置1glm::vec3 cameraPos = glm::vec3(0.0f, 0.0f, 3.0f); 1.2 确定z轴1234// 让摄像机指向场景原点glm::vec3 cameraTarget = glm::vec3(0.0f, 0.0f, 0.0f);// 向量相减，让相机的z轴正方向看向世界的z轴负方向glm::vec3 cameraDirection = glm::normalize(cameraPos - cameraTarget); 1.3 确定x轴1234// 先定义一个上向量glm::vec3 up = glm::vec3(0.0f, 1.0f, 0.0f);// y,z叉乘得到x轴正方向glm::vec3 cameraRight = glm::normalize(glm::cross(up, cameraDirection)); 1.4 确定y轴12// z,x叉乘得到y轴正方向glm::vec3 cameraUp = glm::cross(cameraDirection, cameraRight); 摄像机坐标系如图： 2. GLM构建观察空间矩阵123glm::mat4 view;// 传入摄像机位置，目标方向，和向上向量构建观察矩阵view = glm::lookAt(glm::vec3(0.0f, 0.0f, 3.0f), glm::vec3(0.0f, 0.0f, 0.0f), glm::vec3(0.0f, 1.0f, 0.0f)); 实现相机绕原点旋转： 1234567// 半径float radius = 10.0f;// 画圆float camX = sin(glfwGetTime()) * radius;float camZ = cos(glfwGetTime()) * radius;glm::mat4 view;view = glm::lookAt(glm::vec3(camX, 0.0, camZ), glm::vec3(0.0, 0.0, 0.0), glm::vec3(0.0, 1.0, 0.0)); 效果： 3. 自由移动12345678910111213141516171819202122232425...glm::vec3 cameraPos = glm::vec3(0.0f, 0.0f, 3.0f);glm::vec3 cameraFront = glm::vec3(0.0f, 0.0f, -1.0f); // 看向z轴负方向glm::vec3 cameraUp = glm::vec3(0.0f, 1.0f, 0.0f);// 观察矩阵始终看向z轴负方向view = glm::lookAt(cameraPos, cameraPos + cameraFront, cameraUp);...// 监听键盘输入void processInput(GLFWwindow *window)&#123; ... float cameraSpeed = 0.05f; if (glfwGetKey(window, GLFW_KEY_W) == GLFW_PRESS) cameraPos += cameraSpeed * cameraFront; if (glfwGetKey(window, GLFW_KEY_S) == GLFW_PRESS) cameraPos -= cameraSpeed * cameraFront; if (glfwGetKey(window, GLFW_KEY_A) == GLFW_PRESS) cameraPos -= glm::normalize(glm::cross(cameraFront, cameraUp)) * cameraSpeed; // 减去叉乘得到x轴正方向，往左移动, normalize保证匀速 if (glfwGetKey(window, GLFW_KEY_D) == GLFW_PRESS) cameraPos += glm::normalize(glm::cross(cameraFront, cameraUp)) * cameraSpeed; // 加上叉乘得到x轴正方向，往右移动, normalize保证匀速&#125; 调整移动速度： 根据处理器的能力不同，不能保证不同设备每秒绘制相同帧数。导致配置的不同，有些人可能移动很快，而有些人会移动很慢。实现不同设备匀速： 1234567891011121314151617181920// 当前帧与上一帧的时间差float deltaTime = 0.0f;// 上一帧的时间float lastFrame = 0.0f;...float currentFrame = glfwGetTime();// 更新渲染当前帧花费的时间deltaTime = currentFrame - lastFrame;lastFrame = currentFrame;...void processInput(GLFWwindow *window)&#123; // 使用时间去平衡速度：帧率高的deltaTime小，速度慢，帧率低的deltaTime大，速度快 float cameraSpeed = 2.5f * deltaTime; ...&#125; 效果： 4. 视角移动4.1 欧拉角欧拉角(Euler Angle)是可以表示3D空间中任何旋转的3个值，由莱昂哈德·欧拉(Leonhard Euler)提出。一共有3种欧拉角：俯仰角(Pitch)、偏航角(Yaw)和滚转角(Roll)，如下图： 游戏中的摄像机比较少使用到滚转角，此处推导经过俯仰角和偏航角后摄像机的方向： 可得： 1234// 由相机初始看向x正半轴推导出来的相机新的方向direction.x = cos(glm::radians(pitch)) * cos(glm::radians(yaw));direction.y = sin(glm::radians(pitch));direction.z = cos(glm::radians(pitch)) * sin(glm::radians(yaw)); 可以用用旋转矩阵进行推导，出现结果不一致可参考链接。 4.2 鼠标输入通过鼠标的移动来控制镜头： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// 上一次鼠标位置float lastX = 400, lastY = 300;// 隐藏光标glfwSetInputMode(window, GLFW_CURSOR, GLFW_CURSOR_DISABLED);// 回调函数void mouse_callback(GLFWwindow* window, double xpos, double ypos);// 监听鼠标输入glfwSetCursorPosCallback(window, mouse_callback);void mouse_callback(GLFWwindow* window, double xpos, double ypos)&#123; // 防止首次抖动 if(firstMouse) &#123; lastX = xpos; lastY = ypos; firstMouse = false; &#125; // 计算x,y偏移 float xoffset = xpos - lastX; float yoffset = lastY - ypos; lastX = xpos; lastY = ypos; // 灵敏度 float sensitivity = 0.05; // 偏移乘上灵敏系数，获得合适的改变幅度 xoffset *= sensitivity; yoffset *= sensitivity; // 将偏移加到仰俯，偏航角上 yaw += xoffset; pitch += yoffset; // 限制极限情况(在90度时视角会发生逆转) if(pitch &gt; 89.0f) pitch = 89.0f; if(pitch &lt; -89.0f) pitch = -89.0f; // 使用公式计算新的相机方向 glm::vec3 front; front.x = cos(glm::radians(yaw)) * cos(glm::radians(pitch)); front.y = sin(glm::radians(pitch)); front.z = sin(glm::radians(yaw)) * cos(glm::radians(pitch)); cameraFront = glm::normalize(front);&#125; 4.3 缩放缩放的原理是改变fov视野大小，当视野变小时，场景投影出来的空间就会减小，产生放大的感觉，视野变大则相反，代码： 12345678910111213141516171819void scroll_callback(GLFWwindow* window, double xoffset, double yoffset)// 监听鼠标滚轮输入glfwSetScrollCallback(window, scroll_callback);void scroll_callback(GLFWwindow* window, double xoffset, double yoffset)&#123; if(fov &gt;= 1.0f &amp;&amp; fov &lt;= 45.0f) fov -= yoffset; if(fov &lt;= 1.0f) fov = 1.0f; if(fov &gt;= 45.0f) fov = 45.0f;&#125;// 更新投影矩阵的视野projection = glm::perspective(glm::radians(fov), 800.0f / 600.0f, 0.1f, 100.0f); 效果： 5. 摄像机类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140#ifndef CAMERA_H#define CAMERA_H#include &lt;glad/glad.h&gt;#include &lt;glm/glm.hpp&gt;#include &lt;glm/detail/type_vec.hpp&gt;#include &lt;glm/gtc/matrix_transform.hpp&gt;// 相机移动方向enum Camera_Movement &#123; FORWARD, BACKWARD, LEFT, RIGHT&#125;;// 默认偏航角// 由于公式用的初态相机位置看向x轴正方向，因此偏航-90度，让相机看向z轴负方向const float YAW = -90.0f;// 默仰俯角const float PITCH = 0.0f;// 默认移动速度const float SPEED = 3.0f;// 默认鼠标灵敏系数，影响旋转const float SENSITIVITY = 0.05f;// 默认缩放视野const float ZOOM = 45.0f;class Camera&#123;public: // 位置 glm::vec3 Position; // 向前方向 z轴 glm::vec3 Front; // 向上方向 y轴 glm::vec3 Up; // 向右方向 x轴 glm::vec3 Right; // 世界的上方向 glm::vec3 WorldUp; // 欧拉角 float Yaw; float Pitch; // 其它属性 float MovementSpeed; float MouseSensitivity; double Zoom; // 构造函数 Camera(glm::vec3 position = glm::vec3(0.0f, 0.0f, 0.0f), glm::vec3 up = glm::vec3(0.0f, 1.0f, 0.0f), float yaw = YAW, float pitch = PITCH) : Front(glm::vec3(0.0f, 0.0f, -1.0f)), MovementSpeed(SPEED), MouseSensitivity(SENSITIVITY), Zoom(ZOOM) &#123; Position = position; WorldUp = up; Yaw = yaw; Pitch = pitch; updateCameraVectors(); &#125; Camera(float posX, float posY, float posZ, float upX, float upY, float upZ, float yaw, float pitch) : Front(glm::vec3(0.0f, 0.0f, -1.0f)), MovementSpeed(SPEED), MouseSensitivity(SENSITIVITY), Zoom(ZOOM) &#123; Position = glm::vec3(posX, posY, posZ); WorldUp = glm::vec3(upX, upY, upZ); Yaw = yaw; Pitch = pitch; updateCameraVectors(); &#125; // 返回使用欧拉角的观察矩阵 glm::mat4 GetViewMatrix() &#123; return glm::lookAt(Position, Position + Front, Up); &#125; // 键盘回调 void ProcessKeyboard(Camera_Movement direction, float deltaTime) &#123; float velocity = MovementSpeed * deltaTime; if (direction == FORWARD) Position += Front * velocity; if (direction == BACKWARD) Position -= Front * velocity; if (direction == LEFT) Position -= Right * velocity; if (direction == RIGHT) Position += Right * velocity; &#125; // 鼠标回调 void ProcessMouseMovement(float xoffset, float yoffset, GLboolean constrainPitch = true) &#123; xoffset *= MouseSensitivity; yoffset *= MouseSensitivity; Yaw += xoffset; Pitch += yoffset; // 限制 if (constrainPitch) &#123; if (Pitch &gt; 89.0f) Pitch = 89.0f; if (Pitch &lt; -89.0f) Pitch = -89.0f; &#125; // 使用更新的Euler角度更新“前向量”、“右向量”和“上向量” updateCameraVectors(); &#125; // 鼠标滚轮回调 void ProcessMouseScroll(float yoffset) &#123; Zoom -= (float)yoffset; if (Zoom &lt; 1.0f) Zoom = 1.0f; if (Zoom &gt; 100.0f) Zoom = 100.0f; &#125;private: // 根据摄影机的（更新的）欧拉角计算前向量 void updateCameraVectors() &#123; glm::vec3 front; front.x = cos(glm::radians(Yaw)) * cos(glm::radians(Pitch)); front.y = sin(glm::radians(Pitch)); front.z = sin(glm::radians(Yaw)) * cos(glm::radians(Pitch)); Front = glm::normalize(front); // 同时重新计算向右和向上矢量 // 对向量进行归一化，因为向上或向下看得越多，它们的长度就越接近0，这会导致移动速度变慢 Right = glm::normalize(glm::cross(Front, WorldUp)); Up = glm::normalize(glm::cross(Right, Front)); &#125;&#125;;#endif 6. 扩展阅读如何将欧拉角转换为方向向量 欧拉角导致万向锁原因 欧拉角与万向死锁","categories":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"}],"tags":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"OpenGL","slug":"OpenGL","permalink":"https://raoyuqi.github.io/tags/OpenGL/"}]},{"title":"坐标","slug":"graphic/opengl/坐标","date":"2023-07-26T13:53:31.000Z","updated":"2023-07-26T14:20:18.946Z","comments":false,"path":"2023/07/26/graphic/opengl/坐标/","link":"","permalink":"https://raoyuqi.github.io/2023/07/26/graphic/opengl/%E5%9D%90%E6%A0%87/","excerpt":"","text":"1. 概述对物体的不同操作有不同的意义，你如：当需要对物体进行修改的时候，在局部空间中操作会比较合理；如果要对一个物体做出一个相对于其它物体位置的操作时，在世界坐标系中更合理，等等。因此也衍生出了不同的坐标空间。 2. 坐标空间 局部空间 物体所在的坐标空间，即对象最开始所在的地方。建模软件中创建了一个立方体，立方体的原点有可能位于(0, 0, 0)，有可能创建的所有模型都以(0, 0, 0)为初始位置，然而它们会最终出现在世界的不同位置。所以，模型的所有顶点都是在局部空间中：它们相对于物体来说都是局部的。 世界空间 指顶点相对于游戏世界的坐标。如果希望将物体分散在世界上摆放，就需要将物体变换到世界空间。该变换是由模型矩阵(Model Matrix)实现的。模型矩阵能通过对物体进行位移、缩放、旋转来将它置于它本应该在的位置或朝向。经过模型矩阵变换后，物体的坐标将会从局部变换到世界空间。 观察空间 以摄像机为视角观察到的空间，因此也称摄像机空间。观察空间是将世界空间坐标转化为用户视野前方的坐标而产生的结果，因此观察空间就是从摄像机的视角所观察到的空间。改变换由观察矩阵(View Matrix)实现。 裁剪空间 OpenGL希望所有的坐标都在一个特定的范围内，所有不在范围内的顶点都会被裁剪丢弃，剩下才会进行处理，节省不必要的消耗。将顶点从观察空间变换到裁剪空间需要使用投影矩阵。指定了一个坐标范围，如：-100到100，投影矩阵会将该指定范围转换为标准化设备的范围(-1.0, 1.0)。所有在(-1.0, 1.0)之外的顶点都会被裁剪，比如顶点坐标为(80, 101)会被裁剪，因为转换后y坐标超出了范围，应该丢弃。 如果只是图元(Primitive)，例如三角形，的一部分超出了裁剪体积(Clipping Volume)，则OpenGL会重新构建这个三角形为一个或多个三角形让它能够适合这个裁剪范围。 由投影矩阵创建的观察箱(Viewing Box)被称为平截头体(Frustum)，每个出现在平截头体范围内的坐标都会最终出现在用户的屏幕上。将特定范围内的坐标转化到标准化设备坐标系的过程被称之为投影(Projection)。 当所有顶点被变换到裁剪空间，会执行透视除法，在这个过程中做的就是将位置向量的x，y，z分量分别除以向量的齐次w分量；目的是把4D裁剪空间的齐次坐标变换为3D标准化设备坐标。这一步会在每一个顶点着色器运行的最后被自动执行。 在这一阶段之后，最终的坐标将会被映射到屏幕空间中（即glViewport中的设定），并被变换成片段。投影矩阵有两种： 正交投影 正交投影定义了一个类似立方体的平截头箱，它定义了一个裁剪空间，在这空间之外的顶点都会被裁剪掉。创建一个正射投影矩阵需要指定可见平截头体的宽、高和长度。上面的平截头体定义了可见的坐标，它由由宽、高、近(Near)平面和远(Far)平面所指定。任何出现在近平面之前或远平面之后的坐标都会被裁剪掉。 由于每个向量的w分量都没有进行改变（w分量等于1.0），因此透视除法后坐标不变，出现的视觉效果是，进出的物体和远处的物体大小一致，造成不真实感。 GLM创建正交投影矩阵： 1234// 前两个参数指定了平截头体的左右坐标，第三和第四参数指定了平截头体的底部和顶部// 通过这四个参数定义近平面和远平面的大小，然后第五和第六个参数则定义了近平面和远平面的距离// 这个投影矩阵会将处于这些x，y，z值范围内的坐标变换为标准化设备坐标glm::ortho(0.0f, 800.0f, 0.0f, 600.0f, 0.1f, 100.0f); 透视投影 现实生活中近大远小的现象称为透视，要实现透视效果需要使用透视投影矩阵来完成。透视投影矩阵将给定的平截头体范围映射到裁剪空间，还会修改每个顶点坐标的w值，从而使得离观察者越远的顶点坐标w分量越大。被变换到裁剪空间的坐标都会在-w到w的范围之间（任何不在这个范围的坐标都会被裁剪掉）。OpenGL对在范围内的顶点进行透视除法操作： $out=\\begin{pmatrix} x/w \\\\ y/w \\\\ z/w \\end{pmatrix}$ 由于越远的物体w分量越大，因此模拟除了透视效果，这是w重要用途之一。 GLM创建透视投影矩阵： 1234// 第一个参数定义了fov的值，表示的是视野(Field of View)，想要一个真实的观察效果，它的值通常设置为45.0f，但想要一个末日风格的结果你可以将其设置一个更大的值// 第二个参数设置了宽高比，由视口的宽除以高所得// 第三和第四个参数设置了平截头体的近和远平面。通常设置近距离为0.1f，而远距离设为100.0f。所有在近平面和远平面内且处于平截头体内的顶点都会被渲染glm::mat4 proj = glm::perspective(glm::radians(45.0f), (float)width/(float)height, 0.1f, 100.0f); 当你把透视矩阵的 near 值设置太大时（如10.0f），OpenGL会将靠近摄像机的坐标（在0.0f和10.0f之间）都裁剪掉，这会导致在游戏中的视觉效果：太过靠近一个物体的时候你的视线会直接穿过去。 由于正交投影没有使用透视，远处的物体不会显得更小，所以正射投影主要用于二维渲染以及一些建筑或工程的程序，在这些场景中更希望顶点不会被透视所干扰。 屏幕空间 顶点着色器的输出要求所有的顶点都在裁剪空间内，因此先将顶点从局部空间转换到裁剪空间： $V_{clip}=M_{projection} \\cdot M_{view} \\cdot M_{model} \\cdot V_{local}$ 转换到裁剪空间后，最后会将结果赋值给顶点着色器的gl_Position，OpenGL将会自动进行透视除法和裁剪操作。 OpenGL接着会对裁剪坐标执行透视除法将它们变换到标准化设备坐标。OpenGL使用glViewPort内部的参数将标准化设备坐标映射到屏幕坐标，每个坐标都关联了一个屏幕上的点，这个过程称为视口变换。 物体变换过程： 局部坐标是对象相对于局部原点的坐标，是物体起始的坐标 将局部坐标变换为世界空间坐标，这些坐标相对于世界的全局原点，它们会和其它物体一起相对于世界的原点进行摆放 将世界坐标变换为观察空间坐标，使得每个坐标都是从摄像机或者说观察者的角度进行观察的 坐标到达观察空间之后，需要将其投影到裁剪坐标。裁剪坐标会被处理至-1.0到1.0的范围内，并判断哪些顶点将会出现在屏幕上 最后将裁剪坐标变换为屏幕坐标，这是视口变换(Viewport Transform)的过程。视口变换将位于-1.0到1.0范围的坐标变换到由glViewport函数所定义的坐标范围内。最后变换出来的坐标将会送到光栅器，将其转化为片段 3. 实现3D效果1234567891011121314151617181920212223// 创建模型矩阵// 绕x轴旋转-55度glm::mat4 model;model = glm::rotate(model, glm::radians(-55.0f), glm::vec3(1.0f, 0.0f, 0.0f));// 创建观察矩阵// 往z轴正方向平移3个单位glm::mat4 view;view = glm::translate(view, glm::vec3(0.0f, 0.0f, -3.0f));// 创建透视投影矩阵glm::mat4 projection;projection = glm::perspective(glm::radians(45.0f), screenWidth / screenHeight, 0.1f, 100.0f);// 将矩阵传递到顶点着色器中int modelLoc = glGetUniformLocation(shader.ID, &quot;model&quot;));glUniformMatrix4fv(modelLoc, 1, GL_FALSE, glm::value_ptr(model));int viewLoc = glGetUniformLocation(shader.ID, &quot;view&quot;));glUniformMatrix4fv(viewLoc, 1, GL_FALSE, glm::value_ptr(view));int projectionLoc = glGetUniformLocation(shader.ID, &quot;model&quot;));glUniformMatrix4fv(projectionLoc, 1, GL_FALSE, glm::value_ptr(projection)); 顶点着色器： 12345678910111213#version 330 corelayout (location = 0) in vec3 aPos;...uniform mat4 model;uniform mat4 view;uniform mat4 projection;void main()&#123; // 将顶点变换到裁剪空间中，矩阵乘法要从右向左 gl_Position = projection * view * model * vec4(aPos, 1.0); ...&#125; 效果： 3D立方体 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// 36个顶点数据，包含顶点坐标和纹理float vertices[] = &#123; -0.5f, -0.5f, -0.5f, 0.0f, 0.0f, 0.5f, -0.5f, -0.5f, 1.0f, 0.0f, 0.5f, 0.5f, -0.5f, 1.0f, 1.0f, 0.5f, 0.5f, -0.5f, 1.0f, 1.0f, -0.5f, 0.5f, -0.5f, 0.0f, 1.0f, -0.5f, -0.5f, -0.5f, 0.0f, 0.0f, -0.5f, -0.5f, 0.5f, 0.0f, 0.0f, 0.5f, -0.5f, 0.5f, 1.0f, 0.0f, 0.5f, 0.5f, 0.5f, 1.0f, 1.0f, 0.5f, 0.5f, 0.5f, 1.0f, 1.0f, -0.5f, 0.5f, 0.5f, 0.0f, 1.0f, -0.5f, -0.5f, 0.5f, 0.0f, 0.0f, -0.5f, 0.5f, 0.5f, 1.0f, 0.0f, -0.5f, 0.5f, -0.5f, 1.0f, 1.0f, -0.5f, -0.5f, -0.5f, 0.0f, 1.0f, -0.5f, -0.5f, -0.5f, 0.0f, 1.0f, -0.5f, -0.5f, 0.5f, 0.0f, 0.0f, -0.5f, 0.5f, 0.5f, 1.0f, 0.0f, 0.5f, 0.5f, 0.5f, 1.0f, 0.0f, 0.5f, 0.5f, -0.5f, 1.0f, 1.0f, 0.5f, -0.5f, -0.5f, 0.0f, 1.0f, 0.5f, -0.5f, -0.5f, 0.0f, 1.0f, 0.5f, -0.5f, 0.5f, 0.0f, 0.0f, 0.5f, 0.5f, 0.5f, 1.0f, 0.0f, -0.5f, -0.5f, -0.5f, 0.0f, 1.0f, 0.5f, -0.5f, -0.5f, 1.0f, 1.0f, 0.5f, -0.5f, 0.5f, 1.0f, 0.0f, 0.5f, -0.5f, 0.5f, 1.0f, 0.0f, -0.5f, -0.5f, 0.5f, 0.0f, 0.0f, -0.5f, -0.5f, -0.5f, 0.0f, 1.0f, -0.5f, 0.5f, -0.5f, 0.0f, 1.0f, 0.5f, 0.5f, -0.5f, 1.0f, 1.0f, 0.5f, 0.5f, 0.5f, 1.0f, 0.0f, 0.5f, 0.5f, 0.5f, 1.0f, 0.0f, -0.5f, 0.5f, 0.5f, 0.0f, 0.0f, -0.5f, 0.5f, -0.5f, 0.0f, 1.0f&#125;;// 修改模型矩阵，让立方体随时间旋转model = glm::rotate(model, (float)glfwGetTime() * glm::radians(20.0f), glm::vec3(0.5f, 1.0f, 0.0f));// 开启深度缓冲glEnable(GL_DEPTH_TEST);// 在每次渲染迭代之前清除深度缓冲，否则前一帧的深度信息仍然保存在缓冲中glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);// 绘制glDrawArrays(GL_TRIANGLES, 0, 36); 效果： 5. 更多立方体12345678910111213141516171819202122232425262728// 每个立方体的位置glm::vec3 cubePositions[] = &#123; glm::vec3( 0.0f, 0.0f, 0.0f), glm::vec3( 2.0f, 5.0f, -15.0f), glm::vec3(-1.5f, -2.2f, -2.5f), glm::vec3(-3.8f, -2.0f, -12.3f), glm::vec3( 2.4f, -0.4f, -3.5f), glm::vec3(-1.7f, 3.0f, -7.5f), glm::vec3( 1.3f, -2.0f, -2.5f), glm::vec3( 1.5f, 2.0f, -2.5f), glm::vec3( 1.5f, 0.2f, -1.5f), glm::vec3(-1.3f, 1.0f, -1.5f) &#125;;...glBindVertexArray(VAO);for(unsigned int i = 0; i &lt; 10; i++)&#123; // 赋予不同的模型矩阵 glm::mat4 model; model = glm::translate(model, cubePositions[i]); float angle = 20.0f * i; model = glm::rotate(model, glm::radians(angle), glm::vec3(1.0f, 0.3f, 0.5f)); ourShader.setMat4(&quot;model&quot;, model); glDrawArrays(GL_TRIANGLES, 0, 36);&#125; 效果：","categories":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"}],"tags":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"OpenGL","slug":"OpenGL","permalink":"https://raoyuqi.github.io/tags/OpenGL/"}]},{"title":"变换","slug":"graphic/opengl/变换","date":"2023-07-25T13:32:02.000Z","updated":"2023-07-26T13:52:25.161Z","comments":false,"path":"2023/07/25/graphic/opengl/变换/","link":"","permalink":"https://raoyuqi.github.io/2023/07/25/graphic/opengl/%E5%8F%98%E6%8D%A2/","excerpt":"","text":"1. 向量既有大小又有方向的量，称为向量。向量相等的依据：方向相同且大小相等，如下图起点不同的两个向量 $\\vec{v}$ 和 $\\vec{w}$ 是相等的： 向量数学表示：$\\vec{v}=\\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}$ 2. 向量运算2.1 向量与标量$\\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} + 1 = \\begin{pmatrix} x+1 \\\\ y+1 \\\\ z+1 \\end{pmatrix}$ 2.2 向量取反$-\\vec{v}=-\\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}=\\begin{pmatrix} -x \\\\ -y \\\\ -z \\end{pmatrix}$ 2.3 向量加减$\\vec{v}+\\vec{w}=\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}+\\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\end{pmatrix}=\\begin{pmatrix} 5 \\\\ 7 \\\\ 9 \\end{pmatrix}$ $\\vec{v}-\\vec{w}=\\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}-\\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\end{pmatrix}=\\begin{pmatrix} -3 \\\\ -3 \\\\ -3 \\end{pmatrix}$ 2.4 向量长度和单位向量向量长度：$||\\vec{v}||=\\sqrt{x^2+y^2}$ 单位向量：$\\widehat{n} = \\frac{\\vec{v}}{||\\vec{v}||}$ 2.4 向量相乘 点乘 $\\vec{v} \\cdot \\vec{w} = ||\\vec{v}|| \\cdot ||\\vec{w}|| \\cdot \\cos\\vartheta$ 两个单位向量的点乘结果为两个向量的夹角：$\\vec{v} \\cdot \\vec{w} = 1 \\cdot 1 \\cdot \\cos\\vartheta = \\cos\\vartheta$ 几何意义：判断两个向量方向的相似性，即两个向量是否垂直、平行、方向相反等。 点乘计算：$\\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} \\cdot \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = (0.6 * 0) + (-1 * 1) + (0 * 0) = -1$，反余弦可得两向量夹角180度，方向相反。 叉乘 叉乘会生成一个垂直于两个向量的新向量，叉乘可以用来判断两个向量的位置关系，即一个向量是在另一个向量的右边还是左边。 叉乘计算：$\\begin{pmatrix} A_x \\\\ A_y \\\\ A_z \\end{pmatrix} \\cdot \\begin{pmatrix} B_x \\\\ B_y \\\\ B_z \\end{pmatrix} = \\begin{pmatrix} A_y \\cdot B_z-A_z \\cdot B_y \\\\ A_z \\cdot B_x-A_x \\cdot B_z \\\\ A_x \\cdot B_y-A_y \\cdot B_x \\end{pmatrix}$ 3. 矩阵矩阵就是一个矩形的数字、符号或表达式数组，矩阵中每一项叫做矩阵的元素。 3.1 矩阵相乘条件： 左侧矩阵的列数与右侧矩阵的行数相等，两个矩阵才能相乘 矩阵相乘不遵守交换律，$A \\cdot B \\not= B \\cdot A$ 例： $\\left[\\begin{matrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\\\ \\end{matrix}\\right] \\cdot \\left[\\begin{matrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\\\ \\end{matrix}\\right]=\\left[\\begin{matrix} 1 \\cdot 5+2 \\cdot 7 &amp; 1 \\cdot 6+2 \\cdot 8 \\\\ 3 \\cdot 5+4 \\cdot 7 &amp; 3 \\cdot 6+4 \\cdot 8 \\\\ \\end{matrix}\\right]=\\left[\\begin{matrix} 19 &amp; 22 \\\\ 43 &amp; 50 \\\\ \\end{matrix}\\right]$ 3.2 矩阵乘以向量矩阵可以用来变换向量： 缩放 $\\left[\\begin{matrix} S_1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; S_2 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; S_3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{matrix}\\right] \\cdot \\left(\\begin{matrix} x \\\\ y \\\\ z \\\\ w \\\\ \\end{matrix}\\right)=\\left(\\begin{matrix} S_1 \\cdot x \\\\ S_2 \\cdot y \\\\ S_3 \\cdot z \\\\ w \\\\ \\end{matrix}\\right)$ 平移 $\\left[\\begin{matrix} 1 &amp; 0 &amp; 0 &amp; T_x \\\\ 0 &amp; 1 &amp; 0 &amp; T_y \\\\ 0 &amp; 0 &amp; 1 &amp; T_z \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{matrix}\\right] \\cdot \\left(\\begin{matrix} x \\\\ y \\\\ z \\\\ 1 \\\\ \\end{matrix}\\right)=\\left(\\begin{matrix} x+T_x \\\\ y+T_y \\\\ z+T_z \\\\ 1 \\\\ \\end{matrix}\\right)$ 向量的w分量也叫齐次坐标，可以把x、y和z坐标分别除以w坐标从而将其次坐标转换为3D向量。如果w分类为1，则表示的是坐标，如果w分量为0，则表示的是向量。 其次坐标的一个主要用途是将平移操作由仿射变换转换为线性变换。 旋转 绕x轴：$\\left[\\begin{matrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\cos\\vartheta &amp; -\\sin\\vartheta &amp; 0 \\\\ 0 &amp; \\sin\\vartheta &amp; \\cos\\vartheta &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{matrix}\\right]$ 绕y轴：$\\left[\\begin{matrix} \\cos\\vartheta &amp; 0 &amp; \\sin\\vartheta &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ -\\sin\\vartheta &amp; 0 &amp; \\cos\\vartheta &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{matrix}\\right]$ 绕z轴：$\\left[\\begin{matrix} \\cos\\vartheta &amp; -\\sin\\vartheta &amp; 0 &amp; 0 \\\\ \\sin\\vartheta &amp; \\cos\\vartheta &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{matrix}\\right]$ 3.3 矩阵组合将多个矩阵相乘可以将多个变换组合到一个矩阵中，比如先缩放再平移：$M=Trans \\cdot Scale -&gt; Trans \\cdot Scale \\cdot \\vec{v}=M \\cdot \\vec{v}$，矩阵相乘时，最右边的矩阵会先和向量相乘，所以这边表示的操作是先缩放后平移。 4. GLMGLM是OpenGL Mathematics的缩写，这是一个OpenGL数学库，点击链接进行下载，然后把头文件的根目录复制到includes文件夹就可以使用了，这里用的是低于0.99版本的GLM。 使用GLM库进行平移操作： 1234567891011#include &lt;glm/glm.hpp&gt;#include &lt;glm/gtc/matrix_transform.hpp&gt;#include &lt;glm/gtc/type_ptr.hpp&gt;// 创建一个向量glm::vec4 vec(1.0f, 0.0f, 0.0f, 1.0f);// 创建一个平移矩阵，平移(1, 1, 0)个单位glm::mat4 trans;trans = glm::translate(trans, glm::vec3(1.0f, 1.0f, 0.0f));// 平移操作vec = trans * vec; 将变换应用到图形中： 1234567891011// 创建一个矩阵glm::mat4 trans;// 绕z轴逆时针旋转90度，glm::radians将角度转化为弧度trans = glm::rotate(trans, glm::radians(90.0f), glm::vec3(0.0, 0.0, 1.0));// 缩放0.5倍trans = glm::scale(trans, glm::vec3(0.5, 0.5, 0.5));// 把矩阵传递给顶点着色器unsigned int transformLoc = glGetUniformLocation(shader.ID, &quot;transform&quot;);// GLM的默认布局就是列主序，所以并不需要转置矩阵glUniformMatrix4fv(transformLoc, 1, GL_FALSE, glm::value_ptr(trans)); 顶点着色器： 123456789101112131415#version 330 corelayout (location = 0) in vec3 aPos;layout (location = 1) in vec2 aTexCoord;out vec2 TexCoord;// 矩阵uniform mat4 transform;void main()&#123; // 变换操作 gl_Position = transform * vec4(aPos, 1.0f); TexCoord = vec2(aTexCoord.x, 1.0 - aTexCoord.y);&#125; 运行程序： 让矩形随着时间进行旋转： 123glm::mat4 trans;trans = glm::translate(trans, glm::vec3(0.5f, -0.5f, 0.0f));trans = glm::rotate(trans, (float)glfwGetTime(), glm::vec3(0.0f, 0.0f, 1.0f)); 矩阵的乘法是从右往左的，因此这里会先绕(0, 0, 1)旋转，然后再平移到屏幕右下角，虽然在逻辑上是先平移后旋转。 运行程序： 5. 扩展阅读线性代数本质：中文字幕版本","categories":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"}],"tags":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"OpenGL","slug":"OpenGL","permalink":"https://raoyuqi.github.io/tags/OpenGL/"}]},{"title":"纹理","slug":"graphic/opengl/纹理","date":"2023-07-21T12:56:56.000Z","updated":"2023-07-21T13:20:03.668Z","comments":false,"path":"2023/07/21/graphic/opengl/纹理/","link":"","permalink":"https://raoyuqi.github.io/2023/07/21/graphic/opengl/%E7%BA%B9%E7%90%86/","excerpt":"","text":"1. 纹理的作用通过给顶点输入颜色数据可以让顶点显示指定的颜色，如果想渲染出更真实的图形，那么就需要足够多的顶点数据和颜色数据才能实现，这样开销很大。使用纹理可以解决这个问题。 纹理通常是一张2D图片（也有1D和3D的），是存储物体细节的容器，可以将物体需要的细节数据存储在纹理中，渲染的时候从纹理中采样出所需要的数据，这样就可以让物体非常的同时也不用添加大量的顶点数据。相当于把纹理贴到物体表面，这样物体就有了该纹理的外观，如下图： 想要把纹理映射到三角形上，需要指定每个三角形的每个顶点对应纹理的哪个部分，让每个顶点和纹理坐标关联起来，表示每个顶点该从纹理的哪个部分采样。纹理坐标的范围在0到1之间，使用纹理坐标获取纹理颜色叫做采样(Sampling)。纹理坐标起始于(0, 0)，终始于(1, 1)，下图展示了怎样将纹理映射到三角形： 纹理映射只要给顶点着色器传递纹理坐标就行，它们会被传片段着色器中，片段着色器中会为每个片段进行纹理坐标的插值。 2. 环绕方式纹理坐标范围从(0, 0)到(1, 1)，假设把纹理坐标设置在这个范围之外，应该如何表现？OpenGL提供了以下表现形式： GL_REPEAT: 重复纹理图像 GL_MIRRORED_REPEAT: 镜像重复 GL_CLAMP_TO_EDGE: 超出的部分会重复纹理坐标的边缘，产生边缘被拉伸的效果 GL_CLAMP_TO_BORDER: 超出的坐标为自定义的边缘颜色 视觉效果如下图： 设置代码： 1234567// 设置s(x)轴超出镜像重复glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_MIRRORED_REPEAT);// 设置t(y)轴超出镜像重复glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_MIRRORED_REPEAT);// 指定颜色float borderColor[] = &#123; 1.0f, 1.0f, 0.0f, 1.0f &#125;;glTexParameterfv(GL_TEXTURE_2D, GL_TEXTURE_BORDER_COLOR, borderColor); 3. 过滤纹理是由纹理像素组成，而采样的时候使用的是纹理坐标，所以OpenGL需要知道怎样将纹理像素映射到纹理坐标。有了映射方法才能够根据纹理坐标去查找纹理图像上的像素，然后进行采样提取纹理像素的颜色，最终显示。这里会有一些问题： 纹理分辨率很小：图像上的多个像素在渲染时取纹理映射上取到了同一个点，会有明显的方块状 纹理分辨率过大：图像上的一个像素覆盖的多个纹素，远处出现摩尔纹，进出出现锯齿 因此，需要一些过滤方法进行处理，纹理过滤最重要的两种： GL_NEAREST 邻近过滤，OpenGL的默认过滤方式，当设置为GL_NEAREST的时候，OpenGL会选择中心点最接近纹理坐标的那个像素，如下图： GL_LINEAR 线性过滤，选取纹理坐标附近的n个纹理像素使用插值方法，进行颜色混合。一个纹理像素的中心距离纹理坐标越近对最终的样本颜色的贡献越大，如下图： 两种方式的效果： 线性过滤更够生成更加平滑的效果，通常在图片被放大后可以设置为线性过滤，而初始状态或者别缩小可以设置为邻近过滤节省性能，设置代码： 12glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); 4. Mipmap在透视投影下，远处的物体会更小，如果这时候让它们使用和近处分辨率一样大的纹理，则不合适。由于远处的物体只产生很少的片段，而纹理分辨率太高，导致一个片段会跨越大范围纹理（即像素覆盖一片纹理区域）， 因此OpenGL很难对该片段只拾取一个纹理颜色，导致在小物体上这会产生不真实的感觉；同时高分辨率占用的内存也大，在远处物体上造成内存浪费。 OpenGL使用多级渐远纹理(Mipmap)解决该问题，Mipmap是一系列的纹理图像，后一个纹理图像是前一个的二分之一。思想：对不同距离的物体，使用不同的多级渐远纹理。同时，多级渐远纹理的性能也很好，Mipmap占用内存只是原始纹理的1.33倍，Mipmap如下： OpenGL创建Mipmap： 123glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR_MIPMAP_LINEAR);// 不要将放大的操作设置为Mipmap的过滤选项，这回产生异常，因为纹理放大不会使用MipmapglTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); 5. 纹理的加载与创建5.1 加载纹理Sean Barrett的一个非常流行的单头文件图像加载库，下载链接。加载图片： 123456#define STB_IMAGE_IMPLEMENTATION#include &quot;stb_image.h&quot;// 宽高和颜色通道int width, height, nrChannels;unsigned char *data = stbi_load(&quot;image.jpg&quot;, &amp;width, &amp;height, &amp;nrChannels, 0); 5.2 创建纹理1234567891011121314151617181920// 创建1个纹理unsigned int texture;glGenTextures(1, &amp;texture);// 绑定glBindTexture(GL_TEXTURE_2D, texture);// 为当前绑定的纹理对象设置环绕、过滤方式glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT);glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);// 使用纹理数据生成纹理glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, data);// 生成MipmapglGenerateMipmap(GL_TEXTURE_2D);// 释放数据stbi_image_free(data); 5.3 应用纹理12345678// 在顶点数据中添加纹理坐标用来告诉OpenGL如何对纹理进行采样float vertices[] = &#123;// ---- 位置 ---- ---- 颜色 ---- - 纹理坐标 - 0.5f, 0.5f, 0.0f, 1.0f, 0.0f, 0.0f, 1.0f, 1.0f, // 右上 0.5f, -0.5f, 0.0f, 0.0f, 1.0f, 0.0f, 1.0f, 0.0f, // 右下 -0.5f, -0.5f, 0.0f, 0.0f, 0.0f, 1.0f, 0.0f, 0.0f, // 左下 -0.5f, 0.5f, 0.0f, 1.0f, 1.0f, 0.0f, 0.0f, 1.0f // 左上&#125;; 新的顶点格式： 123// 解析纹理坐标并启用glVertexAttribPointer(2, 2, GL_FLOAT, GL_FALSE, 8 * sizeof(float), (void*)(6 * sizeof(float)));glEnableVertexAttribArray(2); 顶点着色器： 1234567891011121314#version 330 corelayout (location = 0) in vec3 aPos;layout (location = 1) in vec3 aColor;layout (location = 2) in vec2 aTexCoord;out vec3 ourColor;out vec2 TexCoord;void main()&#123; gl_Position = vec4(aPos, 1.0); ourColor = aColor; TexCoord = aTexCoord;&#125; 片段着色器： 1234567891011121314#version 330 coreout vec4 FragColor;in vec3 ourColor;in vec2 TexCoord;// 采样器uniform sampler2D ourTexture;void main()&#123; // 使用纹理坐标在采样器中采样纹理的颜色 FragColor = texture(ourTexture, TexCoord);&#125; 绘制： 123glBindTexture(GL_TEXTURE_2D, texture);glBindVertexArray(VAO);glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0); 运行程序： 6. 纹理单元个纹理的位置值通常称为一个纹理单元，纹理的默认纹理单元是0，它也是默认的激活纹理单元。有了纹理单元就可以在着色器中可以使用多于一个的纹理。通过把纹理单元赋值给采样器可以一次绑定多个纹理，然后激活对应的纹理单元，如下： 123// 在绑定纹理之前先激活纹理单元，纹理单元0默认被激活glActiveTexture(GL_TEXTURE0);glBindTexture(GL_TEXTURE_2D, texture); 修改片段着色器： 123456789101112131415#version 330 core...uniform sampler2D texture1;// 新增uniform sampler2D texture2;void main()&#123; // 根据第三个参数进行线性插值 // 如果第三个值是0.0，它会返回第一个输入 // 如果是1.0，会返回第二个输入值 // 0.2会返回80%的第一个输入颜色和20%的第二个输入颜色，即返回两个纹理的混合色 FragColor = mix(texture(texture1, TexCoord), texture(texture2, TexCoord), 0.2);&#125; 使用第二张纹理： 12345678910111213141516171819202122232425262728// 修改顶点float vertices[] = &#123; // ---- 位置 ---- ---- 颜色 ---- - 纹理坐标 - 0.5f, 0.5f, 0.0f, 1.0f, 0.0f, 0.0f, 2.0f, 2.0f, // 右上 0.5f, -0.5f, 0.0f, 0.0f, 1.0f, 0.0f, 2.0f, 0.0f, // 右下 -0.5f, -0.5f, 0.0f, 0.0f, 0.0f, 1.0f, 0.0f, 0.0f, // 左下 -0.5f, 0.5f, 0.0f, 1.0f, 1.0f, 0.0f, 0.0f, 2.0f // 左上&#125;;// 翻转y轴stbi_set_flip_vertically_on_load(true);// 载入纹理图片...// 绑定和激活glActiveTexture(GL_TEXTURE0);glBindTexture(GL_TEXTURE_2D, texture1);glActiveTexture(GL_TEXTURE1);glBindTexture(GL_TEXTURE_2D, texture2);// 设置uniform变量之要先前激活着色器程序ourShader.use();// 设置采样器对应的纹理单元glUniform1i(glGetUniformLocation(ourShader.ID, &quot;texture1&quot;), 0);glUniform1i(glGetUniformLocation(ourShader.ID, &quot;texture2&quot;), 1); glBindVertexArray(VAO);glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0); 运行程序：","categories":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"}],"tags":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"OpenGL","slug":"OpenGL","permalink":"https://raoyuqi.github.io/tags/OpenGL/"}]},{"title":"着色器","slug":"graphic/opengl/着色器","date":"2023-07-18T13:20:44.000Z","updated":"2023-07-21T13:20:44.233Z","comments":false,"path":"2023/07/18/graphic/opengl/着色器/","link":"","permalink":"https://raoyuqi.github.io/2023/07/18/graphic/opengl/%E7%9D%80%E8%89%B2%E5%99%A8/","excerpt":"","text":"1. 程序结构123456789101112131415#version version_numberin type in_variable_name;in type in_variable_name;out type out_variable_name;uniform type uniform_name;int main()&#123; // 处理输入并进行一些图形操作 ... // 输出处理过的结果到输出变量 out_variable_name = weird_stuff_we_processed;&#125; 2. 输入与输出着色器虽然都是独立的小程序，但是最后经过编译链接后都是一个整体的一部分，所以每个着色器都通过输入和输出进行数据交互。GLSL定义了in和out关键字专门来实现这个目的，遵循原则：输出变量与下一个着色器阶段的输入匹配（类型与变量名完全一致），就会传递下去。 顶点着色器可以使用location指定输入变量，实现可以在CPU上配置顶点属性，从顶点数据中直接接收输入，如layout (location &#x3D; 0) in vec3 pos。 也可以忽略layout (location = 0)标识符，使用glGetAttribLocation查询属性位置值(Location)，在着色器中设置可以节省OpenGL的工作量。 顶点着色器 12345678910#version 330 corelayout (location = 0) in vec3 aPos; // 位置变量的属性位置值为0out vec4 vertexColor; // 为片段着色器指定一个颜色输出void main()&#123; gl_Position = vec4(aPos, 1.0); // 注意我们如何把一个vec3作为vec4的构造器的参数 vertexColor = vec4(0.5, 0.0, 0.0, 1.0); // 把输出变量设置为暗红色&#125; 片段着色器 123456789#version 330 coreout vec4 FragColor;in vec4 vertexColor; // 从顶点着色器传来的输入变量（名称相同、类型相同）void main()&#123; FragColor = vertexColor;&#125; 片段着色器需要输出最终像素颜色，如果没有定义输出颜色会显示黑或白，这里颜色通过顶点着色器发送。vertexColor在两个着色器中类型和变量名完全一致，因此在编译链接着色器程序的过程中，OpenGL会把两个变量链接在一起，使它们可以发送数据。 运行程序： 3. Uniform特点： 支持从CPU发送数据到GPU 变量是全局的，它可以被着色器程序的任意着色器在任意阶段访问 论你把uniform值设置成什么，uniform会一直保存它们的数据，直到被重置或更新 声明了一个uniform却在GLSL代码中没用过，编译器会静默移除这个变量，最后编译出的版本中并不会包含它 修改片段着色器代码： 123456789#version 330 coreout vec4 FragColor;uniform vec4 ourColor; // 在OpenGL程序代码中设定这个变量void main()&#123; FragColor = ourColor;&#125; 在CPU中传递颜色数据： 1234567891011121314151617181920212223242526272829303132while(!glfwWindowShouldClose(window))&#123; // 输入 processInput(window); // 渲染 // 清除颜色缓冲 glClearColor(0.2f, 0.3f, 0.3f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); // 记得激活着色器 glUseProgram(shaderProgram); // 获取运行时间 float timeValue = glfwGetTime(); // 颜色分量 float greenValue = (sin(timeValue) / 2.0f) + 0.5f; // 查询uniform ourColor的位置值，返回-1代表没有找到这个位置值 int vertexColorLocation = glGetUniformLocation(shaderProgram, &quot;ourColor&quot;); // 激活着色器程序 glUseProgram(shaderProgram); // 设置颜色值，更新uniform值之前必须先激活着色器程序 glUniform4f(vertexColorLocation, 0.0f, greenValue, 0.0f, 1.0f); // 绘制三角形 glBindVertexArray(VAO); glDrawArrays(GL_TRIANGLES, 0, 3); // 交换缓冲并查询IO事件 glfwSwapBuffers(window); glfwPollEvents();&#125; 运行程序： 4. 链接更多属性把颜色添加到顶点数据中： 123456float vertices[] = &#123; // 位置 // 颜色 0.5f, -0.5f, 0.0f, 1.0f, 0.0f, 0.0f, // 右下 -0.5f, -0.5f, 0.0f, 0.0f, 1.0f, 0.0f, // 左下 0.0f, 0.5f, 0.0f, 0.0f, 0.0f, 1.0f // 顶部&#125;; 调整顶点着色器： 1234567891011#version 330 corelayout (location = 0) in vec3 aPos; // 位置变量的属性位置值为 0 layout (location = 1) in vec3 aColor; // 颜色变量的属性位置值为 1out vec3 ourColor; // 向片段着色器输出一个颜色void main()&#123; gl_Position = vec4(aPos, 1.0); ourColor = aColor; // 将ourColor设置为我们从顶点数据那里得到的输入颜色&#125; 删除片段着色器中的uniform变量： 12345678#version 330 coreout vec4 FragColor; in vec3 ourColor;void main()&#123; FragColor = vec4(ourColor, 1.0);&#125; 添加了另一个顶点属性，并且更新了VBO的内存后，VBO内存中的数据布局： 解析顶点数据： 123456// 位置属性，位置0glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 6 * sizeof(float), (void*)0);glEnableVertexAttribArray(0);// 颜色属性，位置1，起始偏移3 * sizeof(float)glVertexAttribPointer(1, 3, GL_FLOAT, GL_FALSE, 6 * sizeof(float), (void*)(3 * sizeof(float)));glEnableVertexAttribArray(1); 运行程序： 5. 着色器类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121#ifndef SHADER_H#define SHADER_H#include &lt;glad/glad.h&gt;#include &lt;string&gt;#include &lt;fstream&gt;#include &lt;sstream&gt;#include &lt;iostream&gt;class Shader&#123;public: unsigned int ID; // constructor generates the shader on the fly // ------------------------------------------------------------------------ Shader(const char* vertexPath, const char* fragmentPath) &#123; // 1. retrieve the vertex/fragment source code from filePath std::string vertexCode; std::string fragmentCode; std::ifstream vShaderFile; std::ifstream fShaderFile; // ensure ifstream objects can throw exceptions: vShaderFile.exceptions (std::ifstream::failbit | std::ifstream::badbit); fShaderFile.exceptions (std::ifstream::failbit | std::ifstream::badbit); try &#123; // open files vShaderFile.open(vertexPath); fShaderFile.open(fragmentPath); std::stringstream vShaderStream, fShaderStream; // read file&#x27;s buffer contents into streams vShaderStream &lt;&lt; vShaderFile.rdbuf(); fShaderStream &lt;&lt; fShaderFile.rdbuf(); // close file handlers vShaderFile.close(); fShaderFile.close(); // convert stream into string vertexCode = vShaderStream.str(); fragmentCode = fShaderStream.str(); &#125; catch (std::ifstream::failure&amp; e) &#123; std::cout &lt;&lt; &quot;ERROR::SHADER::FILE_NOT_SUCCESSFULLY_READ: &quot; &lt;&lt; e.what() &lt;&lt; std::endl; &#125; const char* vShaderCode = vertexCode.c_str(); const char * fShaderCode = fragmentCode.c_str(); // 2. compile shaders unsigned int vertex, fragment; // vertex shader vertex = glCreateShader(GL_VERTEX_SHADER); glShaderSource(vertex, 1, &amp;vShaderCode, NULL); glCompileShader(vertex); checkCompileErrors(vertex, &quot;VERTEX&quot;); // fragment Shader fragment = glCreateShader(GL_FRAGMENT_SHADER); glShaderSource(fragment, 1, &amp;fShaderCode, NULL); glCompileShader(fragment); checkCompileErrors(fragment, &quot;FRAGMENT&quot;); // shader Program ID = glCreateProgram(); glAttachShader(ID, vertex); glAttachShader(ID, fragment); glLinkProgram(ID); checkCompileErrors(ID, &quot;PROGRAM&quot;); // delete the shaders as they&#x27;re linked into our program now and no longer necessary glDeleteShader(vertex); glDeleteShader(fragment); &#125; // activate the shader // ------------------------------------------------------------------------ void use() &#123; glUseProgram(ID); &#125; // utility uniform functions // ------------------------------------------------------------------------ void setBool(const std::string &amp;name, bool value) const &#123; glUniform1i(glGetUniformLocation(ID, name.c_str()), (int)value); &#125; // ------------------------------------------------------------------------ void setInt(const std::string &amp;name, int value) const &#123; glUniform1i(glGetUniformLocation(ID, name.c_str()), value); &#125; // ------------------------------------------------------------------------ void setFloat(const std::string &amp;name, float value) const &#123; glUniform1f(glGetUniformLocation(ID, name.c_str()), value); &#125;private: // utility function for checking shader compilation/linking errors. // ------------------------------------------------------------------------ void checkCompileErrors(unsigned int shader, std::string type) &#123; int success; char infoLog[1024]; if (type != &quot;PROGRAM&quot;) &#123; glGetShaderiv(shader, GL_COMPILE_STATUS, &amp;success); if (!success) &#123; glGetShaderInfoLog(shader, 1024, NULL, infoLog); std::cout &lt;&lt; &quot;ERROR::SHADER_COMPILATION_ERROR of type: &quot; &lt;&lt; type &lt;&lt; &quot;\\n&quot; &lt;&lt; infoLog &lt;&lt; &quot;\\n -- --------------------------------------------------- -- &quot; &lt;&lt; std::endl; &#125; &#125; else &#123; glGetProgramiv(shader, GL_LINK_STATUS, &amp;success); if (!success) &#123; glGetProgramInfoLog(shader, 1024, NULL, infoLog); std::cout &lt;&lt; &quot;ERROR::PROGRAM_LINKING_ERROR of type: &quot; &lt;&lt; type &lt;&lt; &quot;\\n&quot; &lt;&lt; infoLog &lt;&lt; &quot;\\n -- --------------------------------------------------- -- &quot; &lt;&lt; std::endl; &#125; &#125; &#125;&#125;;#endif 使用： 12345678Shader ourShader(&quot;path/to/shaders/shader.vs&quot;, &quot;path/to/shaders/shader.fs&quot;);...while(...)&#123; ourShader.use(); ourShader.setFloat(&quot;someUniform&quot;, 1.0f); ...&#125;","categories":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"}],"tags":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"OpenGL","slug":"OpenGL","permalink":"https://raoyuqi.github.io/tags/OpenGL/"}]},{"title":"画三角形","slug":"graphic/opengl/画三角形","date":"2023-07-17T12:46:20.000Z","updated":"2023-07-21T13:20:50.162Z","comments":false,"path":"2023/07/17/graphic/opengl/画三角形/","link":"","permalink":"https://raoyuqi.github.io/2023/07/17/graphic/opengl/%E7%94%BB%E4%B8%89%E8%A7%92%E5%BD%A2/","excerpt":"","text":"1. 图形渲染管线1.1 概念图形渲染管线是个流水线，分为好几个阶段，主要的工作为：将3D空间中的坐标转换为屏幕空间的2D坐标，并最终把2D坐标转换为有颜色的像素，形成画面。 1.2 着色器图形渲染管线分为多个阶段，每个阶段会把前一个阶段的输出作为输入。每个阶段高度专门化，非常容易并行执行，因此大多数显卡都有成千上万个小处理核心，处理核心负责在GPU上为每个流水线阶段处运行各自的程序，达到在渲染管线中快速并行地处理数据，这些小程序就是着色器。OpenGL着色器是用OpenGL着色器语言(OpenGL Shading Language, GLSL)写的。 1.3 阶段如图，蓝色部分是可以自定义的部分： 顶点着色器 以顶点坐标作为输入，将其处理成齐次坐标。 图元装配 将顶点着色器输出的所有顶点作为输入，并将所有的点装配成指定的图元形状（点、三角形等）。 几何着色器 把图元装配形成的顶点集合作为输入，这个阶段可以产生新顶点，构造出新的图元来生成其它形状。 光栅化阶段 几何着色器的输出作为输入，这个阶段会把图元映射为最终屏幕上对应的像素，生成给片段着色器使用的片段。生成片段过程中会执行裁剪，丢弃超出视图以外的所有像素，提升下个阶段的效率。 片段着色器 这里会计算出像素最后显示的颜色，是所有OpenGL高级效果产生的地方。通常，片段着色器包含3D场景的数据（比如光照、阴影、光的颜色等等），这些数据可以被用来计算最终像素的颜色 Alpha测试和混合 这是最后一个阶段，会对片段进行一系列检测，如深度和模板测试，透明度测试及混合等。 现代OpenGL中，至少定义一个顶点着色器和片段着色器。 2. 绘制三角形 准备顶点数据 tip: OpenGL是3D图形库，因此指定的坐标需要是3D坐标；OpenGL只处理标准化设备坐标，即-1.0到1.0范围内的坐标，并不是简单地把3D坐标转换为屏幕上的2D像素。 给定如下输入坐标： 123456// 将每个z坐标设为0，让它看上去是2D的float vertices[] = &#123; -0.5f, -0.5f, 0.0f, 0.5f, -0.5f, 0.0f, 0.0f, 0.5f, 0.0f&#125;; 深度可以理解为z坐标，它代表一个像素在空间中和你的距离，如果被别的像素遮挡， 就看不到它了，将会被丢弃，以节省资源。 该组坐标在标准化设备中对应如下三角形： 接下来把顶点坐标传给渲染管线的第一个阶段（顶点着色器），顶点着色器会在GPU上创建内存，用来存储顶点数据，还要配置OpenGL如何解释这些内存，并且指定其如何发送给显卡。 创建VBO 通过顶点缓冲对象(Vertex Buffer Objects, VBO)管理这个内存，它会在GPU内存（通常被成为显存）中存储大量顶点。使用缓冲对象的好处是可以批量发送数据到显卡上，而不是每次发送一个顶点。从CPU把数据发送到显卡相对较慢，但是当数据发送至显卡的内存中后，顶点着色器几乎能立即访问顶点。 123456789// 创建VBO对象unsigned int VBO;glGenBuffers(1, &amp;VBO);// 绑定到GL_ARRAY_BUFFER，绑定后使用的任何（在GL_ARRAY_BUFFER目标上的）缓冲调用都会用来配置当前绑定的缓冲(VBO)glBindBuffer(GL_ARRAY_BUFFER, VBO);// 把顶点数据复制到缓冲的内存中glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW); 希望显卡如何管理给定的数据的三种形式： - GL_STATIC_DRAW ：数据不会或几乎不会改变 - GL_DYNAMIC_DRAW：数据会被改变很多 - GL_STREAM_DRAW ：数据每次绘制时都会改变 这样顶点数据就被存储到显存中了，被VBO这个顶点缓冲对象所管理着。 着色器 用GLSL编写顶点着色器： 1234567891011// 版本号和核心模式#version 330 core// 设定了输入变量的位置值layout (location = 0)// 接收顶点数据in vec3 aPos;void main()&#123; gl_Position = vec4(aPos.x, aPos.y, aPos.z, 1.0);&#125; 编译着色器 将第3步的着色器代码赋值给字符串： 1234567891011121314151617181920212223242526272829// 顶点着色器代码const char *vertexShaderSource = &quot;#version 330 core\\n&quot; &quot;layout (location = 0) in vec3 aPos;\\n&quot; &quot;void main()\\n&quot; &quot;&#123;\\n&quot; &quot; gl_Position = vec4(aPos.x, aPos.y, aPos.z, 1.0);\\n&quot; &quot;&#125;\\0&quot;;// 创建一个顶点着色器对象unsigned int vertexShader;vertexShader = glCreateShader(GL_VERTEX_SHADER);// 着色器源码附加到着色器对象上glShaderSource(vertexShader, 1, &amp;vertexShaderSource, NULL);// 编译glCompileShader(vertexShader);// 片段着色器代码const char *fragmentShaderSource = &quot;#version 330 core\\n&quot; &quot;layout (location = 0) out vec4 FragColor;\\n&quot; &quot;void main()\\n&quot; &quot;&#123;\\n&quot; &quot; FragColor = vec4(1.0f, 0.5f, 0.2f, 1.0f);\\n&quot; &quot;&#125;\\0&quot;;unsigned int fragmentShader;fragmentShader = glCreateShader(GL_FRAGMENT_SHADER);glShaderSource(fragmentShader, 1, &amp;fragmentShaderSource, NULL);glCompileShader(fragmentShader); 着色器程序 着色器程序对象是多个着色器最终链接完成的版本。如果要使用刚才编译的着色器必须把它们链接(Link)为一个着色器程序对象，然后在渲染对象的时候激活这个着色器程序。已激活着色器程序的着色器将在发送渲染调用的时候被使用。 1234567891011121314// 创建着色器程序对象unsigned int shaderProgram;shaderProgram = glCreateProgram();// 附加着色器对象到程序glAttachShader(shaderProgram, vertexShader);glAttachShader(shaderProgram, fragmentShader);// 链接glLinkProgram(shaderProgram);// 激活着色器程序glUseProgram(shaderProgram);// 释放不再需要的资源glDeleteShader(vertexShader);glDeleteShader(fragmentShader); 到这一步已经把输入顶点数据发送给了GPU，并指示了GPU如何在顶点和片段着色器中处理这些数据。接下来需要告诉OpenGL如何解释内存中的顶点数据，以及它该如何将顶点数据链接到顶点着色器的属性上。 链接顶点属性 必须手动指定输入数据的哪一个部分对应顶点着色器的哪一个顶点属性。所以必须在渲染前指定OpenGL该如何解释顶点数据，顶点缓冲数据会被解析为下面这样子： 1234// 解析顶点数据glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0);// 以顶点属性位置值作为参数，启用顶点属性，默认是禁用的glEnableVertexAttribArray(0); 顶点数组对象VAO 顶点数组对象(Vertex Array Object, VAO)可以像顶点缓冲对象那样被绑定，任何随后的顶点属性调用都会储存在这个VAO中。好处是，当配置顶点属性指针时，只需要将那些调用执行一次，之后再绘制物体的时候只需要绑定相应的VAO就行了，因为设置的所有状态都将存储在VAO中。 123// 创建一个VAO对象unsigned int VAO;glGenVertexArrays(1, &amp;VAO); 当打算绘制多个物体时，首先要生成&#x2F;配置所有的VAO（和必须的VBO及属性指针)，然后储存它们供后面使用。当打算绘制物体的时候就拿出相应的VAO，绑定它，绘制完物体后，再解绑VAO。这段代码应该看起来像这样： 123456789101112131415161718// ..:: 初始化代码（只运行一次 (除非你的物体频繁改变)） :: ..// 1. 绑定VAOglBindVertexArray(VAO);// 2. 把顶点数组复制到缓冲中供OpenGL使用glBindBuffer(GL_ARRAY_BUFFER, VBO);glBufferData(GL_ARRAY_BUFFER, sizeof(vertices), vertices, GL_STATIC_DRAW);// 3. 设置顶点属性指针glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0);glEnableVertexAttribArray(0);// [...]// ..:: 绘制代码（渲染循环中） :: ..// 绘制物体glUseProgram(shaderProgram);glBindVertexArray(VAO);someOpenGLFunctionThatDrawsOurTriangle();// 解绑VAO 绘制三角形 123456// 激活着色器程序glUseProgram(shaderProgram);// 绑定VAOglBindVertexArray(VAO);// 画三角形，顶点数组的起始索引为0，绘制3个顶点glDrawArrays(GL_TRIANGLES, 0, 3); 结果： 3. 元素缓冲对象元素缓冲对象(Element Buffer Object，EBO)，也叫索引缓冲对象(Index Buffer Object，IBO)。假设不绘制一个三角形而是绘制一个矩形。可以绘制两个三角形来组成一个矩形： 12345678910float vertices[] = &#123; // 第一个三角形 0.5f, 0.5f, 0.0f, // 右上角 0.5f, -0.5f, 0.0f, // 右下角 -0.5f, 0.5f, 0.0f, // 左上角 // 第二个三角形 0.5f, -0.5f, 0.0f, // 右下角 -0.5f, -0.5f, 0.0f, // 左下角 -0.5f, 0.5f, 0.0f // 左上角&#125;; 一个矩形只有4个而不是6个顶点，这样会产生50%的额外开销，如果有包括上千个三角形的模型，这会产生一大堆浪费。可以通过只储存不同的顶点，并设定绘制这些顶点的顺序。这样子只要储存4个顶点就能绘制矩形了，之后只要指定绘制的顺序就行。元素缓冲区对象就是用来做这个的。 EBO是一个缓冲区，就像一个顶点缓冲区对象一样，它存储OpenGL用来决定要绘制哪些顶点的顺序。定义不重复的顶点和绘制出矩形所需的索引顺序： 123456789101112131415161718192021222324float vertices[] = &#123; 0.5f, 0.5f, 0.0f, // 右上角 0.5f, -0.5f, 0.0f, // 右下角 -0.5f, -0.5f, 0.0f, // 左下角 -0.5f, 0.5f, 0.0f // 左上角&#125;;// 注意索引从0开始! // 此例的索引(0,1,2,3)就是顶点数组vertices的下标，// 这样可以由下标代表顶点组合成矩形unsigned int indices[] = &#123; 0, 1, 3, // 第一个三角形 1, 2, 3 // 第二个三角形&#125;;// 创建EBO对象unsigned int EBO;glGenBuffers(1, &amp;EBO);// 把索引复制到缓冲里glBufferData(GL_ELEMENT_ARRAY_BUFFER, sizeof(indices), indices, GL_STATIC_DRAW);// 从索引缓冲区渲染三角形glBindBuffer(GL_ELEMENT_ARRAY_BUFFER, EBO);glDrawElements(GL_TRIANGLES, 6, GL_UNSIGNED_INT, 0); 结果： 线框模式 glPolygonMode(GL_FRONT_AND_BACK, GL_LINE)函数配置OpenGL如何绘制图元。 第一个参数表示打算将其应用到所有的三角形的正面和背面，第二个参数表示用线来绘制。 之后的绘制调用会一直以线框模式绘制三角形，直到调用glPolygonMode(GL_FRONT_AND_BACK, GL_FILL)将其设置回默认模式。","categories":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"}],"tags":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"OpenGL","slug":"OpenGL","permalink":"https://raoyuqi.github.io/tags/OpenGL/"}]},{"title":"创建窗口","slug":"graphic/opengl/创建窗口","date":"2023-07-15T01:01:48.000Z","updated":"2023-07-21T13:20:55.630Z","comments":false,"path":"2023/07/15/graphic/opengl/创建窗口/","link":"","permalink":"https://raoyuqi.github.io/2023/07/15/graphic/opengl/%E5%88%9B%E5%BB%BA%E7%AA%97%E5%8F%A3/","excerpt":"","text":"1. 实例化GLFW窗口12345678910111213int main()&#123; glfwInit(); //设置OpenGL主版本号(Major)和次版本号(Minor) glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3); glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 3); //告诉GLFW使用的是核心模式(Core-profile) glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE); //Mac OS X系统的额外设置 //glfwWindowHint(GLFW_OPENGL_FORWARD_COMPAT, GL_TRUE); return 0;&#125; 2. 创建窗口1234567891011121314int main()&#123; //创建窗口 GLFWwindow* window = glfwCreateWindow(800, 600, &quot;Hello Word&quot;, NULL, NULL); if (window == NULL) &#123; std::cout &lt;&lt; &quot;Failed to create GLFW window&quot; &lt;&lt; std::endl; glfwTerminate(); return -1; &#125; //通知GLFW将窗口的上下文设置为当前线程的主上下文了 glfwMakeContextCurrent(window);&#125; 3. 初始化GLADGLAD是用来管理OpenGL的函数指针的，所以在调用任何OpenGL的函数之前我们需要初始化GLAD。 12345678910int main()&#123; //GLAD是用来管理OpenGL的函数指针的 //调用任何OpenGL的函数之前先需要初始化GLAD if (!gladLoadGLLoader((GLADloadproc)glfwGetProcAddress)) &#123; std::cout &lt;&lt; &quot;Failed to initialize GLAD&quot; &lt;&lt; std::endl; return -1; &#125;&#125; 给GLAD传入了用来加载系统相关的OpenGL函数指针地址的函数。GLFW的glfwGetProcAddress根据编译的系统定义了正确的函数。 4. 设置视口必须告诉OpenGL渲染窗口的尺寸大小，即视口(Viewport)，这样OpenGL才只能知道怎样根据窗口大小显示数据和坐标。并且注册窗口大小改变的回调监听。 12345678910111213void framebuffer_size_callback(GLFWwindow* window, int width, int height);int main()&#123; glViewport(0, 0, 800, 600); //告诉GLFW每当窗口调整大小的时候调用framebuffer_size_callback函数更新视口 glfwSetFramebufferSizeCallback(window, framebuffer_size_callback);&#125;void framebuffer_size_callback(GLFWwindow* window, int width, int height)&#123; glViewport(0, 0, width, height);&#125; 5. 渲染循环添加渲染循环，在GLFW退出前一直保持运行。 1234567int main()&#123; //交换缓冲 glfwSwapBuffers(window); //函数检查有没有触发什么事件（比如键盘输入、鼠标移动等）、更新窗口状态，并调用对应的回调函数 glfwPollEvents();&#125; 双缓冲(Double Buffer) 应用程序使用单缓冲绘图时可能会存在图像闪烁的问题。 这是因为生成的图像不是一下子被绘制出来的，而是按照从左到右，由上而下逐像素地绘制而成的。 最终图像不是在瞬间显示给用户，而是通过一步一步生成的，这会导致渲染的结果很不真实。 为了规避这些问题，我们应用双缓冲渲染窗口应用程序。前缓冲保存着最终输出的图像，它会在屏幕上显示；而所有的的渲染指令都会在后缓冲上绘制。当所有的渲染指令执行完毕后，我们交换(Swap)前缓冲和后缓冲，这样图像就立即呈显出来，之前提到的不真实感就消除了。 6. 输入12345678910111213141516171819//监听输入void processInput(GLFWwindow* window);void processInput(GLFWwindow* window)&#123; if (glfwGetKey(window, GLFW_KEY_ESCAPE) == GLFW_PRESS) glfwSetWindowShouldClose(window, true);&#125;int main()&#123; //输入 processInput(window); //交换缓冲 glfwSwapBuffers(window); //函数检查有没有触发什么事件（比如键盘输入、鼠标移动等）、更新窗口状态，并调用对应的回调函数 glfwPollEvents();&#125; 7. 资源释放123456789int main()&#123; //... //渲染循环结束后需要正确释放/删除之前的分配的所有资源 glfwTerminate(); return 0;&#125;","categories":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"}],"tags":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"OpenGL","slug":"OpenGL","permalink":"https://raoyuqi.github.io/tags/OpenGL/"}]},{"title":"OpenGL环境部署","slug":"graphic/opengl/OpenGL环境部署","date":"2023-07-13T13:35:40.000Z","updated":"2023-07-21T13:21:00.893Z","comments":false,"path":"2023/07/13/graphic/opengl/OpenGL环境部署/","link":"","permalink":"https://raoyuqi.github.io/2023/07/13/graphic/opengl/OpenGL%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/","excerpt":"","text":"1. 概述OpenGL本身并不是一个API，它仅仅是一个由Khronos组织制定并维护的规范。基于该规范编程在不同操作系统会有差异，借助库可以节省编写操作系统相关的代码，流行的库有：GLUT，SDL，SFML和GLFW。 2. 部署环境2.1 GLFW一个专门针对OpenGL的C语言库，提供了一些渲染物体所需的最低限度的接口。允许用户创建OpenGL上下文、定义窗口参数以及处理用户输入等。 从官网下载并将其解压，接着创建一个build目录作为最终导出目录，然后使用CMake工具生成工程文件。 下载CMake并安装，执行cmake-gui，如下设置： 点击Generate按钮，生成的工程文件会在build文件夹中。 在build文件夹里可以找到GLFW.sln文件，用Visual Studio 2019打开。因为CMake已经配置好了项目，并按照默认配置将其编译为64位的库，所以直接点击Build Solution(生成解决方案)，然后在build&#x2F;src&#x2F;Debug文件夹内就会出现我们编译出的库文件glfw3.lib。 2.2 GLFW链接建立一个新的目录用来存放所有的第三方库文件和头文件，建议包含Libs和Include文件夹，在这里存放OpenGL工程用到的所有第三方库和头文件。 打开Visual Studio，创建一个新的项目。选择Visual C++，然后选择空项目，接着将项目从x86更改为x64。为了使程序能够使用GLFW，需要把GLFW库链接进工程： 在Windows平台，opengl32.lib已经包含在Microsoft SDK里了，它在Visual Studio安装的时候就默认安装了。只需将opengl32.lib添加进连接器设置里就行了。OpenGL库64位版本的文件名仍然是opengl32.lib（和32位版本一样）。 2.3 GLAD由于OpenGL只是一个标准&#x2F;规范，具体的实现是由驱动开发商针对特定显卡实现的。OpenGL驱动版本众多，它大多数函数的位置都无法在编译时确定下来，需要在运行时查询。开发者需要在运行时获取函数地址并将其保存在一个函数指针中供以后使用。取得地址的方法因平台而异，在Windows上类似这样： 1234567// 定义函数原型typedef void (*GL_GENBUFFERS) (GLsizei, GLuint*);// 找到正确的函数并赋值给函数指针GL_GENBUFFERS glGenBuffers = (GL_GENBUFFERS)wglGetProcAddress(&quot;glGenBuffers&quot;);// 现在函数可以被正常调用了GLuint buffer;glGenBuffers(1, &amp;buffer); 使用GLAD库可以简化这个过程。 打开GLAD的在线服务，如下设置： 点击Generate按钮来生成库文件，下载GLAD生成的压缩包并解压，将两个头文件目录（glad和KHR）复制到Include文件夹中，并添加glad.c文件到工程中，到此环境配置就完成了。","categories":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"}],"tags":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"OpenGL","slug":"OpenGL","permalink":"https://raoyuqi.github.io/tags/OpenGL/"}]},{"title":"Hello World","slug":"hello-world","date":"2023-07-11T13:30:41.175Z","updated":"2023-07-11T13:30:41.175Z","comments":true,"path":"2023/07/11/hello-world/","link":"","permalink":"https://raoyuqi.github.io/2023/07/11/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"Uniy","slug":"Uniy","permalink":"https://raoyuqi.github.io/categories/Uniy/"},{"name":"性能优化","slug":"Uniy/性能优化","permalink":"https://raoyuqi.github.io/categories/Uniy/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/categories/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"}],"tags":[{"name":"图形学","slug":"图形学","permalink":"https://raoyuqi.github.io/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"},{"name":"Uniy","slug":"Uniy","permalink":"https://raoyuqi.github.io/tags/Uniy/"},{"name":"性能优化","slug":"性能优化","permalink":"https://raoyuqi.github.io/tags/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/"},{"name":"合并Mesh","slug":"合并Mesh","permalink":"https://raoyuqi.github.io/tags/%E5%90%88%E5%B9%B6Mesh/"},{"name":"Mix","slug":"Mix","permalink":"https://raoyuqi.github.io/tags/Mix/"},{"name":"Dots","slug":"Dots","permalink":"https://raoyuqi.github.io/tags/Dots/"},{"name":"OpenGL","slug":"OpenGL","permalink":"https://raoyuqi.github.io/tags/OpenGL/"}]}